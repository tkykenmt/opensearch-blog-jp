---
title: "[翻訳] Lucene-on-Faiss: OpenSearch の高性能・省メモリなベクトル検索を支える技術"
emoji: "🔍"
type: "tech"
topics: ["opensearch", "vectorsearch", "faiss", "lucene"]
published: true
published_at: 2025-09-15
publication_name: "opensearch"
---

:::message
本記事は [OpenSearch Project Blog](https://opensearch.org/blog/) に投稿された以下の記事を日本語に翻訳したものです。
:::

https://opensearch.org/blog/lucene-on-faiss-powering-opensearchs-high-performance-memory-efficient-vector-search/

[ベクトル検索](https://opensearch.org/vector-search/)は、セマンティック検索、レコメンデーションシステム、生成 AI の検索拡張生成 (RAG) など、現代のアプリケーションを支える基盤技術です。高次元空間におけるベクトル間の位置関係を比較することで、類似したアイテムを見つけ出すことができます。しかし、既存のベクトル検索エンジンにはいくつかの制約があります。例えば、強力な近似最近傍探索 (ANN) ライブラリである Faiss は、インデックス全体をメモリに載せる必要があり、スケーラビリティに制約があります。一方、Lucene のベクトル検索はメモリ効率に優れていますが、性能面では Faiss に及びません。

これらのトレードオフを解消するため、私たちは両者の長所を組み合わせた *Lucene-on-Faiss* というハイブリッドアプローチを開発しました。Lucene の検索エンジンから Faiss の高性能な HNSW グラフインデックスを直接利用できるようにすることで、検索スループットを 2 倍に向上させながら、Faiss のメモリ制約を克服しました。本記事では、この統合の仕組みと、OpenSearch のベクトル検索ユースケースにおける優れたパフォーマンス結果について詳しく解説します。

## 最高のベクトルデータベースを目指して - ハイブリッドアプローチ

OpenSearch は、期待に応えるだけでなく、それを超えるベクトルデータベースを目指しています。この目標を達成するため、業界で最も実績のあるライブラリである Faiss と Lucene の強みを組み合わせました。ハイブリッド設計の詳細に入る前に、Faiss と Lucene のエンジンがどのように動作するかを見ていきましょう。

### Faiss の仕組み

Facebook AI Research が開発した Faiss は、C++ で書かれた高性能な類似検索ライブラリで、SIMD 命令を使用してベクトル間の距離計算を効率的に行います。プロダクト量子化 (PQ)、FP16 サポート、GPU ベースのインデックス作成など、強力な機能を備えており、大規模で高スループットなアプリケーションに最適です。

しかし、Faiss はインデックス全体をメモリにロードする必要があります。エンジンはベクトルデータのサイズと同じ連続したオフヒープメモリブロックを確保します。例えば、10 GB のフラットインデックスには 10 GB の物理メモリが必要です。そのため、メモリに制約のある環境では Faiss を運用できません。例えば、16 GB の RAM を持つシステムでは、16 GB を超える Faiss インデックスを格納することは不可能であり、大幅なハードウェア投資なしにスケーラビリティを確保することが困難です。

### Lucene の仕組み

Lucene は Java で書かれた高度に最適化された検索エンジンライブラリで、最近ベクトル検索のネイティブサポートが追加され、従来のキーワード検索やフィルタクエリと並行して類似検索が可能になりました。Faiss とは異なり、Lucene はベクトルデータの部分的なロードが可能で、スコアベースの検索もサポートしています。この設計により、特にメモリに制約のある環境で、Lucene はより柔軟でリソース効率の高い動作が可能です。この効率性の鍵となるのが Lucene の **Directory** 抽象化で、これによりデータの読み書き方法を制御できます。ユーザーは性能とリソース要件に応じて異なる Directory 実装を選択できます。

例えば、**NIOFSDirectory** はリクエストごとにファイルシステムから直接バイトを読み取り、レイテンシは高くなりますがメモリ使用量を抑えられます。一方、OpenSearch のデフォルトである **MMapDirectory** は、OS のメモリマッピングを使用してファイルにアクセスし、I/O オーバーヘッドを削減しますがメモリ消費は増加します。この設定の柔軟性により、ユーザーは Lucene のベクトル検索をハードウェア制約に合わせて調整でき、Faiss の全メモリロード方式では難しい低メモリ環境での運用が可能になります。

さらに、Lucene の HNSW グラフアルゴリズムの実装には、Faiss にはない最適化が含まれています。並行検索における早期終了機能では、スレッド間で最小適格スコアを共有して収束を高速化し、フィルタリング時により多くのベクトルを訪問することで再現率を向上させています。

しかし、Lucene は距離計算において複数の SIMD 最適化 (例えば、バルク XOR 演算や FP16、BF16 形式のサポート) が不足しています。これらは Faiss が得意とする領域です。

ハイブリッドアプローチでは、Faiss の SIMD を活用した距離計算を使用しながら、検索には Lucene の効率的な HNSW アルゴリズムを引き続き使用できます。

### Lucene-on-Faiss - Lucene と Faiss の統合

Faiss は SIMD を使用した距離計算により高速なベクトル検索を実現します。一方、Lucene は並行検索における早期終了や、必要な部分のみをロードする機能に優れています。

ハイブリッドアプローチでは、Faiss のインデックスを Lucene の HNSW アルゴリズムに組み込むことで、Lucene の部分ロードと早期終了機能を活用しながら、距離計算は Faiss の最適化されたコードに任せることにしました。この統合により、Faiss インデックスを使用しながらもメモリ制約下でベクトル検索を実行でき、さらに Lucene の並行 HNSW 最適化の恩恵も受けられるようになりました。

最初のステップとして、以下の図に示すように、Faiss で構築されたインデックス上で Lucene の HNSW アルゴリズムを正常に実行することに成功しました。

![Faiss インデックス上で Lucene の HNSW アルゴリズムを実行](/images/opensearch-lucene-on-faiss-vector-search/running-lucene-hnsw-algorithm-on-faiss-index.png)

統合の残りの部分は現在も進行中で、まもなく完了する予定です。

ユーザーの視点からは、このハイブリッド構成は完全に透過的です。インデックス設定で有効・無効を切り替えるだけで、OpenSearch がバックグラウンドで残りの処理を行います。この機能を有効にするには、以下のリクエストを使用し、`index.knn.memory_optimized_search` を `true` に設定します (デフォルトは `false`)。

```json
PUT /my_index
{
  "settings" : {
    "index.knn": true,
    "index.knn.memory_optimized_search" : true 
  },
  "mappings": {
    <Index fields>
  }
}
```

[OpenSearch バージョン 3.1](https://opensearch.org/blog/get-started-with-opensearch-3-1/) では、1x 圧縮でオンディスクモードを有効にすると、フィールドレベルでこのハイブリッドアプローチも有効になります。将来のリリースでは、オンディスクモードのすべての圧縮レベルでこのアプローチをデフォルトにする予定です。

```json
PUT /my_index
{
  "mappings": {
    "properties": {
      "my_field": {
        <Other info>
        "mode": "on_disk",
        "compression_level": "1x"
      }
    }
  }
}
```

Lucene-on-Faiss は、1x、2x、4x、16x、32x 圧縮を含むすべての HNSW 構成でサポートされています。現在のリリースでは、IVF や PQ などのトレーニングベースの手法はサポートされていません。

## パフォーマンス結果

ベクトル検索のパフォーマンスは、利用可能なメモリ量とインデックス構成によって大きく異なります。以下のセクションでは、異なる設定における Faiss と Lucene を使用した OpenSearch のパフォーマンスを紹介します。

### t2.large インスタンスで 30 GB の Faiss インデックスを実行

新しい設定では、インデックス全体をメモリにロードする必要がないため、メモリに制約のある環境でも Faiss エンジンを使用したベクトル検索が可能になります。例えば、Cohere-10m データセットの Faiss インデックスは約 30 GB です。以前は、8 GB のメモリしかない t2.large インスタンスで実行しようとすると、メモリ不足により OpenSearch プロセスが終了していました。

新しい設定を有効にすると、厳しいメモリ制約下でも OpenSearch が正常に動作します。必要なベクトルがすべてメモリに保持されないため、一部はディスクから読み取る必要があり、全メモリ構成と比較するとパフォーマンスは低下します。

### 検索 QPS の比較

同じ環境ですべてのバイトがメモリにロードされた場合、クエリ毎秒 (QPS) のパフォーマンスはどのように変化するでしょうか。

Faiss C++ は、バルク SIMD を使用した高速な距離計算により、FP32 と FP16 で引き続き優れたパフォーマンスを発揮します。一方、Java では FP16 から FP32 への変換オーバーヘッドが発生します。FP32 と FP16 の両方で、再現率がわずかに向上しました。これは、Faiss が HNSW グラフを探索する際に固定回数の訪問を使用するのに対し、Lucene は関連する候補がなくなるまで探索を続けるためと考えられます。

量子化シナリオ (8x、16x、32x) では、Lucene-on-Faiss がより優れたパフォーマンスを達成し、再現率の低下もわずか (最大 4.5%) に抑えられています。これは Lucene の早期終了ロジックによるもので、将来のリリースで改善を予定しています。k の値が増加するにつれて、QPS の差は広がります。32x 量子化で k = 100 の場合、Lucene-on-Faiss は QPS をほぼ 2 倍にします。これは、Lucene-on-Faiss がメモリ制約のある環境だけでなく、高い量子化レベルでも大きな可能性を持つことを示しています。

### ベンチマーク結果

以下の表は、異なるインデックスタイプにおける Faiss C++ と Lucene-on-Faiss アプローチを比較し、最大 QPS、P99 レイテンシ、再現率、および Lucene-on-Faiss の相対的な QPS 改善と再現率の変化を示しています。

テスト環境は以下の通りです。

* データセット: Cohere-10M
* メトリクス: コサイン類似度
* FP32: r6g.4xlarge 3 ノード、6 シャード
* FP16: r6g.2xlarge 3 ノード、6 シャード
* 8x、16x、32x 量子化: r7gd.2xlarge 3 ノード、6 シャード

表のメトリクスに関する注意点は以下の通りです。

* **QPS 変化**が -9.56% の場合、Lucene-on-Faiss のパフォーマンスが 9.56% 低下したことを示し、+51.52% の場合は 51.52% 向上したことを示します。
* **再現率変化**列では、正の値 (例: 0.14%) は再現率の向上を示し、負の値 (例: -4.52%) は低下を示します。

### k=30

| インデックスタイプ | Faiss C++ 最大 QPS | Faiss C++ P99 レイテンシ (ms) | Faiss C++ 再現率 (%) | Lucene-on-Faiss 最大 QPS | Lucene-on-Faiss P99 レイテンシ (ms) | Lucene-on-Faiss 再現率 (%) | QPS 変化 (%) | 再現率変化 (%) |
|---|---|---|---|---|---|---|---|---|
| FP32 | 5419.73 | 5 | 85.01 | 4901.74 | 5.6 | 85.13 | -9.56% | 0.14% |
| FP16 | 3157.60 | 4.9 | 85.11 | 1881.10 | 6.2 | 85.37 | -40.43% | 0.31% |
| 8x 量子化 | 404.04 | 8.7 | 85.29 | 714.55 | 9.1 | 82.94 | 76.85% | -2.76% |
| 16x 量子化 | 395.71 | 8.7 | 85.39 | 732.47 | 7.2 | 82.42 | 85.10% | -3.48% |
| 32x 量子化 | 539.90 | 7.6 | 83.18 | 818.05 | 6.8 | 79.42 | 51.52% | -4.52% |

### k=100

| インデックスタイプ | Faiss C++ 最大 QPS | Faiss C++ P99 レイテンシ (ms) | Faiss C++ 再現率 (%) | Lucene-on-Faiss 最大 QPS | Lucene-on-Faiss P99 レイテンシ (ms) | Lucene-on-Faiss 再現率 (%) | QPS 変化 (%) | 再現率変化 (%) |
|---|---|---|---|---|---|---|---|---|
| FP32 | 3242.32 | 5.8 | 92.42 | 2667.47 | 6.8 | 92.78 | -17.73% | 0.39% |
| FP16 | 1908.30 | 7 | 90.74 | 982.71 | 8.5 | 92.26 | -48.50% | 1.68% |
| 8x 量子化 | 174.82 | 13.3 | 87.86 | 411.80 | 8.4 | 87.17 | 135.56% | -0.79% |
| 16x 量子化 | 169.87 | 13 | 87.9 | 431.62 | 8.9 | 86.96 | 154.09% | -1.07% |
| 32x 量子化 | 227.95 | 13.3 | 87.4 | 472.47 | 7.9 | 85.9 | 107.27% | -1.72% |

## 今後の予定

現在の構成には、まだ大きな改善の余地があります。現時点では、SIMD は FP32 ベクトルに対して内部的に使用されていますが、FP16 やバイナリベクトルには使用されていません。今後のバージョンでは、すべてのベクトルタイプに対して Faiss の SIMD 最適化を有効にする予定です。例えば、FP16 ベクトルは現在 FP32 に変換する必要がありますが、SIMD を使用すれば Java で直接距離を計算でき、速度が向上します。他のデータタイプも SIMD 統合の恩恵を受け、パフォーマンスがさらに向上します。

さらに、パフォーマンスをより向上させるため、明示的なオフヒープベクトルロードの検討も進めています。全体として、ベクトル検索をより高速かつ効率的にする機会は数多くあり、これらの改善を OpenSearch に継続的に取り込んでいきます。

この機能の進捗に興味がある方や参加したい方は、[この GitHub issue](https://github.com/opensearch-project/OpenSearch/issues/9568) で開発状況をフォローし、議論に参加してください。
