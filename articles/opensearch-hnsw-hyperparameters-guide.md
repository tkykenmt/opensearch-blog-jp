---
title: "[ç¿»è¨³] HNSW ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠã®å®Ÿè·µã‚¬ã‚¤ãƒ‰"
emoji: "ğŸ”"
type: "tech"
topics: ["opensearch", "vectorsearch", "hnsw", "machinelearning"]
published: true
published_at: 2025-04-10
publication_name: "opensearch"
---

:::message
æœ¬è¨˜äº‹ã¯ [OpenSearch Project Blog](https://opensearch.org/blog/) ã«æŠ•ç¨¿ã•ã‚ŒãŸä»¥ä¸‹ã®è¨˜äº‹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸã‚‚ã®ã§ã™ã€‚
:::

https://opensearch.org/blog/a-practical-guide-to-selecting-hnsw-hyperparameters/

ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯ã€å¤šãã®æ©Ÿæ¢°å­¦ç¿’ (ML) ã‚„ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã¾ã™ã€‚å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (LLM) ã®æ–‡è„ˆã§ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯ [Retrieval-Augmented Generation (RAG)](https://aws.amazon.com/what-is/retrieval-augmented-generation/) ã‚’æ”¯ãˆã‚‹æŠ€è¡“ã§ã™ã€‚RAG ã¯å¤§è¦æ¨¡ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€LLM ã®å¿œç­”ã‚’æ”¹å–„ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦å³å¯†ãª k è¿‘å‚ (k-NN) ã‚’æ±‚ã‚ã‚‹ã“ã¨ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŸã‚ã€åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã« [Hierarchical Navigable Small Worlds (HNSW)](https://arxiv.org/pdf/1603.09320) ãªã©ã®è¿‘ä¼¼æœ€è¿‘å‚ (ANN) æ¤œç´¢æ‰‹æ³•ãŒã‚ˆãä½¿ç”¨ã•ã‚Œã¾ã™ [1]ã€‚

## HNSW ã®æœ€é©åŒ–ï¼šæ¤œç´¢å“è³ªã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹

HNSW ã‚’åŠ¹æœçš„ã«è¨­å®šã™ã‚‹ã“ã¨ã¯ã€è¤‡æ•°ã®ç›®çš„ã‚’åŒæ™‚ã«æœ€é©åŒ–ã™ã‚‹å•é¡Œã§ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€ä»¥ä¸‹ã® 2 ã¤ã®ä¸»è¦ãªç›®çš„ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚

- **æ¤œç´¢å“è³ª**: recall@k ã§æ¸¬å®šã€‚ä¸Šä½ k ä»¶ã®çœŸã®è¿‘å‚ã®ã†ã¡ã€HNSW ãŒè¿”ã™ k ä»¶ã®çµæœã«å«ã¾ã‚Œã‚‹å‰²åˆã€‚
- **æ¤œç´¢é€Ÿåº¦**: ã‚¯ã‚¨ãƒªã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§æ¸¬å®šã€‚1 ç§’ã‚ãŸã‚Šã«å®Ÿè¡Œã•ã‚Œã‚‹ã‚¯ã‚¨ãƒªæ•°ã€‚

ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“ã‚„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã‚‚é‡è¦ã§ã™ãŒã€ã“ã‚Œã‚‰ã¯ä»Šå¾Œã®è¨˜äº‹ã§å–ã‚Šä¸Šã’ã‚‹äºˆå®šã§ã™ã€‚

HNSW ã‚°ãƒ©ãƒ•ã®æ§‹é€ ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦åˆ¶å¾¡ã•ã‚Œã€ãƒ™ã‚¯ãƒˆãƒ«é–“ã®æ¥ç¶šå¯†åº¦ãŒæ±ºã¾ã‚Šã¾ã™ã€‚å¯†ãªã‚°ãƒ©ãƒ•ã¯ä¸€èˆ¬çš„ã« recall ã‚’å‘ä¸Šã•ã›ã¾ã™ãŒã‚¯ã‚¨ãƒªã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’ä½ä¸‹ã•ã›ã€ç–ãªã‚°ãƒ©ãƒ•ã¯ãã®é€†ã®åŠ¹æœãŒã‚ã‚Šã¾ã™ã€‚é©åˆ‡ãªãƒãƒ©ãƒ³ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹ã«ã¯è¤‡æ•°ã®è¨­å®šã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€ã“ã‚Œã‚’åŠ¹ç‡çš„ã«è¡Œã†æ–¹æ³•ã«ã¤ã„ã¦ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¯é™ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

## ä¸»è¦ãª HNSW ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

HNSW ã§æœ€ã‚‚é‡è¦ãª 3 ã¤ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

- **`M`** â€“ ãƒ™ã‚¯ãƒˆãƒ«ã‚ãŸã‚Šã®ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸ã®æœ€å¤§æ•°ã€‚å€¤ãŒå¤§ãã„ã»ã©ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¾ã™ãŒã€æ¤œç´¢å“è³ªãŒå‘ä¸Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
- **`efSearch`** â€“ æ¤œç´¢æ™‚ã®å€™è£œã‚­ãƒ¥ãƒ¼ã®ã‚µã‚¤ã‚ºã€‚å€¤ãŒå¤§ãã„ã»ã©æ¤œç´¢å“è³ªãŒå‘ä¸Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€æ¤œç´¢ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå¢—åŠ ã—ã¾ã™ã€‚
- **`efConstruction`** â€“ `efSearch` ã¨åŒæ§˜ã§ã™ãŒã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚å€¤ãŒå¤§ãã„ã»ã©æ¤œç´¢å“è³ªãŒå‘ä¸Šã—ã¾ã™ãŒã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“ãŒå¢—åŠ ã—ã¾ã™ã€‚

## åŠ¹æœçš„ãªè¨­å®šã®è¦‹ã¤ã‘æ–¹

ã“ã‚Œã‚‰ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ 1 ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€**ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ– (HPO)** ã§ã™ã€‚ã“ã‚Œã¯ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹é–¢æ•°ã®æœ€é©ãªè¨­å®šã‚’æ¢ç´¢ã™ã‚‹è‡ªå‹•åŒ–æŠ€è¡“ã§ã™ [5, 6]ã€‚ã—ã‹ã—ã€HPO ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ãã€ç‰¹ã«åŸºç¤ã¨ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒã‚ˆãç†è§£ã•ã‚Œã¦ã„ã‚‹å ´åˆã«ã¯ã€é™ã‚‰ã‚ŒãŸåŠ¹æœã—ã‹å¾—ã‚‰ã‚Œãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ [3]ã€‚

ä»£æ›¿æ‰‹æ®µã¨ã—ã¦ **è»¢ç§»å­¦ç¿’** ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€é©åŒ–ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸçŸ¥è­˜ã‚’åˆ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ç”¨ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€åŠ¹ç‡æ€§ã‚’ç¶­æŒã—ãªãŒã‚‰æœ€é©ãªçµæœã«è¿‘ã„è¨­å®šã‚’ç‰¹å®šã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ [3, 4]ã€‚

## æ¨å¥¨ã•ã‚Œã‚‹ HNSW è¨­å®š

æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€HNSW è¨­å®šã‚’é¸æŠã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«åŸºã¥ãã€ã‚°ãƒ©ãƒ•å¯†åº¦ã‚’æ®µéšçš„ã«å¢—åŠ ã•ã›ã‚‹ **5 ã¤ã®äº‹å‰è¨ˆç®—ã•ã‚ŒãŸè¨­å®š** ã‚’æä¾›ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®è¨­å®šã¯ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§æ¤œç´¢å“è³ªã¨é€Ÿåº¦ã®ã•ã¾ã–ã¾ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã¾ã™ã€‚

æ¤œç´¢ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–ã™ã‚‹ã«ã¯ã€**ã“ã‚Œã‚‰ 5 ã¤ã®è¨­å®šã‚’é †ç•ªã«è©•ä¾¡** ã—ã€recall ãŒè¦ä»¶ã‚’æº€ãŸã—ãŸæ™‚ç‚¹ã§åœæ­¢ã§ãã¾ã™ã€‚è¨­å®šã¯æ¤œç´¢å“è³ªãŒå‘ä¸Šã™ã‚‹é †ã«ä¸¦ã‚“ã§ã„ã‚‹ãŸã‚ã€ã“ã®é †åºã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã¨å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚ˆã‚Šè‰¯ã„æ¤œç´¢å“è³ªãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ããªã‚Šã¾ã™ã€‚

```
{'M': 16,  'efConstruction': 128, 'efSearch': 32}
{'M': 32,  'efConstruction': 128, 'efSearch': 32}
{'M': 16,  'efConstruction': 128, 'efSearch': 128}
{'M': 64,  'efConstruction': 128, 'efSearch': 128}
{'M': 128, 'efConstruction': 256, 'efSearch': 256}
```

## HNSW ã®ãŸã‚ã®ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå­¦ç¿’

ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå­¦ç¿’ [2, 3, 4] ã¯ã€ç•°ãªã‚‹ã‚·ãƒŠãƒªã‚ªã§è©•ä¾¡ã—ãŸéš›ã«å¹³å‡çš„ã«å°‘ãªãã¨ã‚‚ 1 ã¤ãŒè‰¯å¥½ã«æ©Ÿèƒ½ã™ã‚‹ã‚ˆã†ãªã€ç›¸è£œçš„ãªè¨­å®šã®ã‚»ãƒƒãƒˆã‚’é¸æŠã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ HNSW ã«é©ç”¨ã—ã€recall ã¨ã‚¯ã‚¨ãƒªã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹è¨­å®šã®ã‚»ãƒƒãƒˆã‚’ç‰¹å®šã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã—ãŸã€‚

ã“ã‚Œã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€ã•ã¾ã–ã¾ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã€åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã€è·é›¢é–¢æ•°ã«ã¾ãŸãŒã‚‹ 15 ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¾ã—ãŸ (ä¸‹è¡¨å‚ç…§)ã€‚å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã€å³å¯†ãª k-NN æ¤œç´¢ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆå†…ã®ã™ã¹ã¦ã®ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹ä¸Šä½ 10 ä»¶ã®æœ€è¿‘å‚ã‚’è¨ˆç®—ã—ã€ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã‚’ç¢ºç«‹ã—ã¾ã—ãŸã€‚

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ                                                                                                     | æ¬¡å…ƒæ•° | è¨“ç·´ã‚µã‚¤ã‚º | ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º | è¿‘å‚æ•° | è·é›¢          | åŸ‹ã‚è¾¼ã¿                      | ãƒ‰ãƒ¡ã‚¤ãƒ³                  |
| ---------------------------------------------------------------------------------------------------------------- | ------ | ---------- | ------------ | ------ | ------------- | ----------------------------- | ------------------------- |
| [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)                                                | 784    | 60,000     | 10,000       | 100    | Euclidean     | â€“                             | ç”»åƒã€è¡£é¡                |
| [MNIST](http://yann.lecun.com/exdb/mnist/)                                                                       | 784    | 60,000     | 10,000       | 100    | Euclidean     | â€“                             | ç”»åƒã€æ•°å­—                |
| [GloVe](https://nlp.stanford.edu/projects/glove/)                                                                | 25     | 1,183,514  | 10,000       | 100    | Angular       | å˜èª-å˜èªå…±èµ·è¡Œåˆ—             | è¨€èª (wiki, common crawl) |
| [GloVe](https://nlp.stanford.edu/projects/glove/)                                                                | 50     | 1,183,514  | 10,000       | 100    | Angular       | å˜èª-å˜èªå…±èµ·è¡Œåˆ—             | è¨€èª (wiki, common crawl) |
| [GloVe](https://nlp.stanford.edu/projects/glove/)                                                                | 100    | 1,183,514  | 10,000       | 100    | Angular       | å˜èª-å˜èªå…±èµ·è¡Œåˆ—             | è¨€èª (wiki, common crawl) |
| [GloVe](https://nlp.stanford.edu/projects/glove/)                                                                | 200    | 1,183,514  | 10,000       | 100    | Angular       | å˜èª-å˜èªå…±èµ·è¡Œåˆ—             | è¨€èª (wiki, common crawl) |
| [NY Times](https://archive.ics.uci.edu/dataset/164/bag+of+wordsD199572657/)                                      | 256    | 290,000    | 10,000       | 100    | Angular       | BoW                           | è¨€èªã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹        |
| [NY Times](https://archive.ics.uci.edu/dataset/164/bag+of+wordsD199572657/)                                      | 16     | 290,000    | 10,000       | 100    | Angular       | BoW                           | è¨€èªã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹        |
| [SIFT](http://corpus-texmex.irisa.fr/)                                                                           | 128    | 1,000,000  | 10,000       | 100    | Euclidean     | SIFT è¨˜è¿°å­                   | ç”»åƒ                      |
| [SIFT](https://github.com/erikbern/ann-benchmarks/tree/main)                                                     | 256    | 1,000,000  | 10,000       | 100    | Hamming       | SIFT è¨˜è¿°å­                   | ç”»åƒ                      |
| [Last.fm](http://millionsongdataset.com/lastfm/)                                                                 | 65     | 292,385    | 50,000       | 100    | Inner product | è¡Œåˆ—åˆ†è§£                      | æ¥½æ›²ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³    |
| [Word2bits](https://github.com/agnusmaximus/Word2BitsD199572657/)                                                | 800    | 399,000    | 1,000        | 100    | Hamming       | é‡å­åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ã Word2Vec | è¨€èªã€è‹±èª Wikipedia      |
| [GIST](http://corpus-texmex.irisa.fr/)                                                                           | 960    | 1,000,000  | 1,000        | 100    | Euclidean     | GIST è¨˜è¿°å­ã€INRIA C å®Ÿè£…     | ç”»åƒ                      |
| [MS MARCO](https://microsoft.github.io/msmarco/)                                                                 | 384    | 1,000,000  | 50,000       | 100    | Euclidean     | MiniLLM                       | è¨€èªã€è³ªå•å¿œç­”            |
| [openai-dbpedia](https://huggingface.co/datasets/Qdrant/dbpedia-entities-openai3-text-embedding-3-large-1536-1M) | 1,536  | 950,000    | 50,000       | 100    | Euclidean     | text-embedding-3-large        | è¨€èªã€DBPedia             |

å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®æ¢ç´¢ç©ºé–“ã«åŸºã¥ã 80 ã® HNSW è¨­å®šã®ã‚°ãƒªãƒƒãƒ‰ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚

```
search_space = {
    "M": [8, 16, 32, 64, 128],
    "efConstruction": [32, 64, 128, 256],
    "efSearch": [32, 64, 128, 256]
}
```

ã“ã‚Œã‚‰ã®å®Ÿé¨“ã§ã¯ã€3 ã¤ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãƒãƒ¼ãƒ‰ã¨ 6 ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ãƒ‰ (å„ãƒãƒ¼ãƒ‰ã¯ `r6g.4xlarge.search` ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹) ã§æ§‹æˆã•ã‚Œã‚‹ OpenSearch 2.15 ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚ãƒ†ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã‚’ 100 ä»¶ã®ãƒãƒƒãƒã§è©•ä¾¡ã—ã€å„ HNSW è¨­å®šã®ã‚¯ã‚¨ãƒªã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¨ recall@10 ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

### æ‰‹æ³•

recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ç•°ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã€ã‚·ãƒ³ãƒ—ãƒ«ãªç·šå½¢åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä½¿ç”¨ã—ã€recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ä¸¡æ–¹ã« 0 ã‹ã‚‰ 1 (ä¸¡ç«¯ã‚’å«ã‚€) ã®å€¤ã‚’å‰²ã‚Šå½“ã¦ã¾ã—ãŸã€‚ç‰¹å®šã®é‡ã¿ä»˜ã‘ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ä»¥ä¸‹ã® 4 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ç·šå½¢åŒ–ã•ã‚ŒãŸç›®çš„ã‚’æœ€å¤§åŒ–ã™ã‚‹è¨­å®šã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚

1. **recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®æ­£è¦åŒ–** â€“ å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§ min-max ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é©ç”¨ã—ã€recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®å€¤ã‚’æ¯”è¼ƒå¯èƒ½ã«ã—ã¾ã™ã€‚
2. **é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—** â€“ å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸé‡ã¿ã‚’ä½¿ç”¨ã—ã¦ã€æ­£è¦åŒ–ã•ã‚ŒãŸ recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æ–°ã—ã„é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã«çµåˆã—ã¾ã™ã€‚
3. **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§ã®å¹³å‡åŒ–** â€“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¹³å‡ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
4. **æœ€è‰¯ã®è¨­å®šã‚’é¸æŠ** â€“ å¹³å‡é‡ã¿ä»˜ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æœ€å¤§åŒ–ã™ã‚‹è¨­å®šã‚’ç‰¹å®šã—ã¾ã™ã€‚

ä»¥ä¸‹ã®å›³ã¯ã€2 ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ 3 ã¤ã®è¨­å®šã‚’ä½¿ç”¨ã—ãŸä¾‹ã§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚

![ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ](/images/opensearch-hnsw-hyperparameters-guide/hnsw-portfolio-learn.png)

recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã«å¯¾ã—ã¦ä»¥ä¸‹ã®é‡ã¿ä»˜ã‘ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚ã»ã¨ã‚“ã©ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æœ€é©åŒ–ã™ã‚‹å‰ã«è‰¯å¥½ãª recall ã‚’é”æˆã™ã‚‹ã“ã¨ã‚’å„ªå…ˆã™ã‚‹ãŸã‚ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã«é«˜ã„é‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã¾ã›ã‚“ã§ã—ãŸã€‚

|                | 0   | 1   | 2   | 3   | 4   |
| -------------- | --- | --- | --- | --- | --- |
| `w_recall`     | 0.9 | 0.8 | 0.7 | 0.6 | 0.5 |
| `w_throughput` | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 |

## è©•ä¾¡

2 ã¤ã®ã‚·ãƒŠãƒªã‚ªã§æ‰‹æ³•ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚

1. **Leave-one-out è©•ä¾¡** â€“ 15 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡ 1 ã¤ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦ä½¿ç”¨ã—ã€æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ã‚»ãƒƒãƒˆã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚
2. **ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆè©•ä¾¡** â€“ 15 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã™ã¹ã¦ã‚’è¨“ç·´ã«ä½¿ç”¨ã—ã€è¨“ç·´ã‚»ãƒƒãƒˆã«å«ã¾ã‚Œã¦ã„ãªã„æ–°ã—ã„åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« [Cohere-embed-english-v3](https://huggingface.co/Cohere/Cohere-embed-english-v3.0) ã‚’ä½¿ç”¨ã—ãŸ 4 ã¤ã®è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æ‰‹æ³•ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

æœ€åˆã®ã‚·ãƒŠãƒªã‚ªã¯æ©Ÿæ¢°å­¦ç¿’ã«ãŠã‘ã‚‹äº¤å·®æ¤œè¨¼ã‚’æ¨¡å€£ã—ã€2 ç•ªç›®ã®ã‚·ãƒŠãƒªã‚ªã¯å®Œå…¨ãªè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸæœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã§ã®è©•ä¾¡ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¾ã™ã€‚

### Leave-one-out è©•ä¾¡

ã“ã®è©•ä¾¡ã§ã¯ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«æ‰‹æ³•ã‚’é©ç”¨ã—ã¦ç•°ãªã‚‹é‡ã¿ä»˜ã‘ã§ã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹è¨­å®šã‚’æ±ºå®šã—ã¾ã—ãŸã€‚æ¬¡ã«ã€åŒã˜æ‰‹æ³•ã‚’ä½¿ç”¨ã—ã¦è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å°å‡ºã•ã‚ŒãŸäºˆæ¸¬è¨­å®šã¨æ¯”è¼ƒã—ã¾ã—ãŸã€‚

æ­£è¦åŒ–ã•ã‚ŒãŸ (min-max ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚ŒãŸ) recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã«ã¤ã„ã¦ã€äºˆæ¸¬è¨­å®šã¨ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹è¨­å®šã®é–“ã®å¹³å‡çµ¶å¯¾èª¤å·® (MAE) ã‚’è¨ˆç®—ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®æ£’ã‚°ãƒ©ãƒ•ã¯ã€leave-one-out è©•ä¾¡ã«ãŠã‘ã‚‹ 15 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®å¹³å‡ MAE ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

![MAE çµæœ](/images/opensearch-hnsw-hyperparameters-guide/mae.png)

çµæœã¯ã€æ­£è¦åŒ–ã•ã‚ŒãŸ recall ã®å¹³å‡ MAE ãŒ 0.1 æœªæº€ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® recall å€¤ãŒ 0.5 ã‹ã‚‰ 0.95 ã®ç¯„å›²ã§ã‚ã‚‹å ´åˆã€0.1 ã® MAE ã¯ç”Ÿã® recall å·®ãŒã‚ãšã‹ 0.045 ã§ã‚ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€äºˆæ¸¬è¨­å®šãŒã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹è¨­å®šã«è¿‘ã„ã“ã¨ã‚’ç¤ºã—ã¦ãŠã‚Šã€ç‰¹ã«é«˜ recall ã®é‡ã¿ä»˜ã‘ã§é¡•è‘—ã§ã™ã€‚

ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã® MAE ã¯ã‚ˆã‚Šå¤§ãããªã£ã¦ã„ã¾ã™ãŒã€ã“ã‚Œã¯ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®šãŒ recall æ¸¬å®šã‚ˆã‚Šã‚‚ãƒã‚¤ã‚ºãŒå¤šã„å‚¾å‘ãŒã‚ã‚‹ãŸã‚ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ãŸã ã—ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã«é«˜ã„é‡ã¿ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã¨ MAE ã¯æ¸›å°‘ã—ã¾ã™ã€‚

### ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆè©•ä¾¡

ã“ã®è©•ä¾¡ã§ã¯ã€15 ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«æ‰‹æ³•ã‚’é©ç”¨ã—ã€Cohere-embed-english-v3 åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸ 3 ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§çµæœã®è¨­å®šã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸã€‚ç›®æ¨™ã¯ã€å­¦ç¿’ã•ã‚ŒãŸè¨­å®šãŒ recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ç•°ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è¡¨ã™ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒˆã«æ²¿ã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§ã—ãŸã€‚

ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒƒãƒˆã¯ã€å­¦ç¿’ã•ã‚ŒãŸè¨­å®šã® recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’ç•°ãªã‚‹è‰²ã§ç¤ºã—ã€ä»–ã®è¨­å®šã¯ã‚°ãƒ¬ãƒ¼ã§è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚

![ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•çµæœ](/images/opensearch-hnsw-hyperparameters-guide/tradeoff.png)

çµæœã¯ã€é¸æŠã•ã‚ŒãŸ 5 ã¤ã®è¨­å®šãŒé«˜ recall ãŠã‚ˆã³é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé ˜åŸŸã‚’åŠ¹æœçš„ã«ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã«é«˜ã„é‡ã¿ã‚’å‰²ã‚Šå½“ã¦ãªã‹ã£ãŸãŸã‚ã€å­¦ç¿’ã•ã‚ŒãŸè¨­å®šã¯ä½ recallãƒ»é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé ˜åŸŸã«ã¯åŠã‚“ã§ã„ã¾ã›ã‚“ã€‚

## OpenSearch ã§ã®è¨­å®šã®é©ç”¨æ–¹æ³•

ã“ã‚Œã‚‰ã®è¨­å®šã‚’è©¦ã™ã«ã¯ã€ã¾ãšã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å‹•çš„ã§ã¯ãªã„ãŸã‚ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆæ™‚ã«æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```bash
curl -X PUT "localhost:9200/test-index" -H 'Content-Type: application/json' -d'
{
  "settings" : {
    "knn": true
  },
  "mappings": {
    "properties": {
      "my_vector": {
        "type": "knn_vector",
        "dimension": 4,
        "space_type": "l2",
        "method": {
          "name": "hnsw",
          "parameters": {
            "m": 16,
            "ef_construction": 256
          }
        }
      }
    }
  }
}
'
```

æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚’æŠ•å…¥ã—ã¾ã™ã€‚

```bash
curl -X PUT "localhost:9200/_bulk" -H 'Content-Type: application/json' -d'
{ "index": { "_index": "test-index" } }
{ "my_vector": [1.5, 5.5, 4.5, 6.4]}
{ "index": { "_index": "test-index" } }
{ "my_vector": [2.5, 3.5, 5.6, 6.7]}
{ "index": { "_index": "test-index" } }
{ "my_vector": [4.5, 5.5, 6.7, 3.7]}
{ "index": { "_index": "test-index" } }
{ "my_vector": [1.5, 5.5, 4.5, 6.4]}
'
```

æœ€å¾Œã«ã€æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```bash
curl -X GET "localhost:9200/test-index/_search?pretty&_source_excludes=my_vector" -H 'Content-Type: application/json' -d'
{
  "size": 100,
  "query": {
    "knn": {
      "my_vector": {
        "vector": [0, 0, 0, 0],
        "k": 100,
        "method_parameters": {
          "ef_search": 128
        }
      }
    }
  }
}
'
```

`ef_search` ã¯æ¤œç´¢æ™‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ãŸã‚ã€å„æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å‹•çš„ã«è¨­å®šã§ãã¾ã™ã€‚

### Python ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ãŸã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ä¾‹

ä»¥ä¸‹ã¯ã€[boto3](https://pypi.org/project/boto3/) ã¨ [opensearch-py](https://pypi.org/project/opensearch-py/) ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½¿ç”¨ã—ãŸ Python ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ä¾‹ã§ã™ã€‚

#### å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿

```python
from typing import Tuple, List
import sys
import time
import logging
import random
import hashlib
import json

import h5py
import numpy as np
import pandas as pd
from tqdm import tqdm
import boto3

from opensearchpy import OpenSearch, RequestsHttpConnection, helpers
from opensearchpy.exceptions import RequestError, NotFoundError, TransportError
from opensearchpy.helpers.errors import BulkIndexError
from requests_aws4auth import AWS4Auth
```

#### ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°ã®ä¿®æ­£

ä»¥ä¸‹ã®é–¢æ•°ã¯ã€`"documents"`ã€`"queries"`ã€`"ground_truth"` ã®ã‚­ãƒ¼ã‚’æŒã¤ `hdf5` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚

```python
def load_data(local_file_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    local_file_path ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€

    Args:
        local_file_path (str): ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹

    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray]:
        ä»¥ä¸‹ã‚’å«ã‚€ã‚¿ãƒ—ãƒ«:
          - documents (np.ndarray): æ¤œç´¢å¯¾è±¡ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚»ãƒƒãƒˆ (n, m)
          - querys (np.ndarray): ANN ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã®ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã‚»ãƒƒãƒˆ (q, m)
          - neighbors (np.ndarray): å„ã‚¯ã‚¨ãƒªã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ä¸Šä½ k è¿‘å‚ã‚’å«ã‚€é…åˆ— (q, k)
    """
    hdf5_file = h5py.File(local_file_path, "r")
    vectors = hdf5_file["documents"]
    query_vectors = hdf5_file["queries"]
    neighbors = hdf5_file["ground_truth"]
    return vectors, query_vectors, neighbors
```

#### ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã®èª­ã¿è¾¼ã¿

```python
logger = logging.getLogger(__name__)


def get_client(host: str, region: str, profile: str) -> OpenSearch:
    """æŒ‡å®šã•ã‚ŒãŸãƒ›ã‚¹ãƒˆã¨ AWS ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ OpenSearch ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹ã€‚
    AWS èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã™ã‚‹ã€‚

    Args:
        host (str): OpenSearch ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
        region (str): AWS ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ (ä¾‹: us-west-2)

    Returns:
        OpenSearch: OpenSearch ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    """
    credentials = boto3.Session(profile_name=profile, region_name=region).get_credentials()

    awsauth = AWS4Auth(
        credentials.access_key,
        credentials.secret_key,
        region,
        "es",
        session_token=credentials.token,
    )

    client = OpenSearch(
        hosts=[{"host": host, "port": 443}],
        http_auth=awsauth,
        use_ssl=True,
        verify_certs=True,
        connection_class=RequestsHttpConnection,
        timeout=60 * 60 * 5,
        search_timeout=60 * 60 * 5,
    )
    return client


def create_index_body(config, engine):
    return {
        "settings": {
            "index": {"knn": True, "knn.algo_param.ef_search": config["efSearch"]},
            "number_of_shards": 1,
            "number_of_replicas": 0,
        },
        "mappings": {
            "_source": {"excludes": ["vector"], "recovery_source_excludes": ["vector"]},
            "properties": {
                "vector": {
                    "type": "knn_vector",
                    "dimension": config["dim"],
                    "method": {
                        "name": "hnsw",
                        "space_type": config["space"],
                        "engine": engine,
                        "parameters": {
                            "ef_construction": config["efConstruction"],
                            "m": config["M"],
                        },
                    },
                }
            },
        },
    }


def get_index_name(config: dict) -> str:
    """è¨­å®šè¾æ›¸ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ã—ã¦ä¸€æ„ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã‚’å–å¾—ã™ã‚‹

    Args:
        config (dict): è©•ä¾¡ã™ã‚‹ HNSW è¨­å®š

    Returns:
        str: ãƒãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ãŸä¸€æ„ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å
    """
    dict_str = "_".join(map(str, config.values()))
    hash_obj = hashlib.md5(dict_str.encode())
    index_name = hash_obj.hexdigest()
    return index_name


def random_delay(lower_time_limit: float = 1.0, upper_time_limit: float = 2.0) -> float:
    return min(lower_time_limit + random.random() * upper_time_limit, upper_time_limit)


def bulk_index_vectors(
    client: OpenSearch,
    index: str,
    vectors: List[np.ndarray],
    source_name: str,
    batch_size=1000,
):
    """batch_size ã§ãƒ™ã‚¯ãƒˆãƒ«ã‚’ index_name ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒãƒ«ã‚¯æŠ•å…¥ã™ã‚‹

    Args:
        client (OpenSearch): OpenSearch ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        index (str): ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ•å…¥ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        vectors (List[np.ndarray]): æŠ•å…¥ã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
        source_name (str): `_source` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ä½¿ç”¨ã™ã‚‹åå‰
        batch_size (int, optional): ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 1000
    """
    actions = []
    for i, vector in enumerate(
        tqdm(vectors, desc="Indexing vectors", total=len(vectors), file=sys.stdout)
    ):
        action = {
            "_index": index,
            "_id": i,
            "_source": {source_name: vector.tolist()},
        }
        actions.append(action)

        if len(actions) == batch_size:
            helpers.bulk(client, actions)
            actions = []

    if actions:
        helpers.bulk(client, actions)


def delete_one_index(client: OpenSearch, index: str, max_retry: int = 5):
    try:
        client.indices.clear_cache(
            index=index, fielddata=True, query=True, request=True
        )
    except NotFoundError:
        pass

    success = False
    count = 0
    while not success and count < max_retry:
        try:
            client.indices.delete(index=index)
            success = True
        except NotFoundError:
            logger.error(f"{index} not found, SKIP.")
            success = True
        except RequestError as e:
            delay = random_delay()
            logger.error(f"{index} delete failed {e}, wait {delay} seconds.")
            time.sleep(delay)
        count += 1


def get_graph_memory(client: OpenSearch) -> float:
    resp = client.transport.perform_request(
        method="GET", url=f"/_plugins/_knn/stats?pretty"
    )
    return sum([stat["graph_memory_usage"] for node_id, stat in resp["nodes"].items()])


def knn_bulk_search(client, config, index_name, query_vectors, k):
    msearch_body = ""
    for query_vector in query_vectors:
        search_header = '{"index": "' + index_name + '"}\n'
        search_body = {
            "size": k,
            "query": {
                "knn": {
                    "vector": {
                        "vector": query_vector.tolist(),
                        "k": k,
                    }
                }
            },
            "_source": False,
        }
        msearch_body += search_header + json.dumps(search_body) + "\n"

    response = client.msearch(body=msearch_body)
    return response


def pad_list(input_list, n):
    """
    å…¥åŠ›ãƒªã‚¹ãƒˆã®é•·ã•ãŒ n æœªæº€ã®å ´åˆã€å³å´ã« -1 ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã€‚
    """
    if len(input_list) < n:
        input_list += [-1] * (n - len(input_list))
    return input_list


def batch_knn_search(client, config, index_name, query_vectors, batch_size, k):
    pred_inds = []
    took_time = 0.0
    for i in tqdm(
        range(0, len(query_vectors), batch_size), desc="Search vectors", file=sys.stdout
    ):
        batch = query_vectors[i : i + batch_size]
        results = knn_bulk_search(client, config, index_name, batch, k)
        for j, result in enumerate(results["responses"]):
            ids = [hit["_id"] for hit in result["hits"]["hits"]]
            if len(ids) < k:
                logger.error(f"{config} batch {i} search needs padding")
                ids = pad_list(ids, k)
            pred_inds.append(ids)
        took_time += results["took"]
    return pred_inds, took_time * 0.001


def compute_recall(labels: np.ndarray, pred_labels: np.ndarray):
    assert labels.shape[0] == pred_labels.shape[0], (
        labels.shape,
        pred_labels.shape,
    )
    assert labels.shape[1] == pred_labels.shape[1], (
        labels.shape,
        pred_labels.shape,
    )
    labels = labels.astype(int)
    pred_labels = pred_labels.astype(int)
    k = labels.shape[1]
    correct = 0
    for pred, truth in zip(pred_labels, labels):
        top_k_pred, truth_k = pred[:k], truth[:k]
        for p in top_k_pred:
            for y in truth_k:
                if p == y:
                    correct += 1
    return float(correct) / (k * labels.shape[0])


def ingest_vectors(
    config: dict,
    engine: str,
    client: OpenSearch,
    index_name: str,
    vectors: List[np.ndarray],
):
    index_body = create_index_body(config, engine)

    success = False
    max_try = 0
    while not success and max_try < 5:
        try:
            client.indices.create(index=index_name, body=index_body)
            logger.info(f"{index_name}: Ingesting vectors")
            bulk_index_vectors(client, index_name, vectors, "vector")
            success = True
        except BulkIndexError as e:
            delay = random_delay()
            delete_one_index(client, index_name)
            max_try += 1
            time.sleep(random_delay())
            logger.error(
                f"{index_name}: BulkIndexError, retrying after {delay} seconds\n{e}"
            )
        except RequestError as e:
            if e.error == "resource_already_exists_exception":
                delay = random_delay()
                logger.error(f"{e}, delete and retry after {delay} seconds")
                delete_one_index(client, index_name)
                time.sleep(random_delay())
                max_try += 1
            else:
                raise


def query_index(config, index_name, client, query_vectors, k) -> list[np.ndarray]:
    success = False
    batch_size = 100
    max_try = 0
    while not success and max_try < 5:
        try:
            pred_inds, search_time = batch_knn_search(
                client, config, index_name, query_vectors, batch_size=batch_size, k=k
            )
            success = True
            return pred_inds, search_time
        except TransportError as e:
            delay = random_delay()
            logger.error(
                f"{index_name}: Query failed, retrying after {delay} seconds {e}"
            )
            time.sleep(delay)
            max_try += 1
    raise Exception(f"{index_name}: Query failed after {max_try} retries")


def eval_config(
    config: dict, local_file_path: str, host: str, region: str, aws_profile: str, engine: str, k=10
):

    vectors, query_vectors, neighbors = load_data(local_file_path)
    client = get_client(host, region, aws_profile)
    index_name = get_index_name(config)

    ingest_vectors(config, engine, client, index_name, vectors)

    client.transport.perform_request(method="POST", url=f"/{index_name}/_refresh")
    client.transport.perform_request(
        method="GET", url=f"/_plugins/_knn/warmup/{index_name}?pretty"
    )
    stats = client.indices.stats(index=index_name, metric="store")
    index_size_in_bytes = stats["indices"][index_name]["total"]["store"][
        "size_in_bytes"
    ]
    graph_mem_in_kb = get_graph_memory(client)

    time.sleep(random_delay(lower_time_limit=5, upper_time_limit=10))

    logger.info(f"{index_name}: Query indexes")
    pred_inds, search_time = query_index(config, index_name, client, query_vectors, k)

    groundtruth_topk_neighbors = [v[:k] for v in neighbors]
    recall = compute_recall(np.array(groundtruth_topk_neighbors), np.array(pred_inds))
    logger.info(f"{index_name}: Recall {recall}")

    config.update(
        {
            f"recall@{k}": recall,
            "search_time": search_time,
            "search_throughput": len(query_vectors) / search_time,
            "index_size_in_bytes": index_size_in_bytes,
            "graph_mem_in_kb": graph_mem_in_kb,
        }
    )

    delete_one_index(client, index_name)
    logger.info(f"Clean up done, finishing evaluation.")
    return config
```

#### å®Ÿé¨“ç”¨å¤‰æ•°ã®å®šç¾©

å®Ÿé¨“ç”¨ã«ä»¥ä¸‹ã®å¤‰æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚

- OpenSearch ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ã‚¨ãƒ³ã‚¸ãƒ³
- AWS ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã¨ AWS ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¬¡å…ƒæ•°
- HNSW ã§ä½¿ç”¨ã™ã‚‹ç©ºé–“

```python
host = "your_domain_endpoint_without_https://"
engine = "faiss"

region = "us-west-2"
aws_profile = "your_aws_profile"

local_file_path = "your_data_path"
dim = 384  # ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°
space = "l2"  # HNSW ã®ç©ºé–“
```

#### ç•°ãªã‚‹è¨­å®šã®è©•ä¾¡

```python
metrics = []
for i, config in enumerate([
    {'M': 16, 'efConstruction': 128, 'efSearch': 32},
    {'M': 32, 'efConstruction': 128, 'efSearch': 32},
    {'M': 16, 'efConstruction': 128, 'efSearch': 128},
    {'M': 64, 'efConstruction': 128, 'efSearch': 128},
    {'M': 128, 'efConstruction': 256, 'efSearch': 256}
]):
    config.update({"dim": dim, "space": space})
    metric = eval_config(config, local_file_path, host, region, aws_profile, engine)
    metrics.append(metric)
```

ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã«ä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

```
Indexing vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8674/8674 [00:26<00:00, 321.72it/s]
Search vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:09<00:00,  1.55it/s]
Indexing vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8674/8674 [00:34<00:00, 248.95it/s]
Search vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:07<00:00,  1.88it/s]
Indexing vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8674/8674 [00:30<00:00, 280.84it/s]
Search vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:07<00:00,  2.09it/s]
Indexing vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8674/8674 [00:27<00:00, 311.41it/s]
Search vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:07<00:00,  2.07it/s]
Indexing vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8674/8674 [00:34<00:00, 250.86it/s]
Search vectors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:07<00:00,  2.03it/s]
```

ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å¯è¦–åŒ–ã§ãã¾ã™ã€‚

```python
df = pd.DataFrame(metrics)
df
```

ä»¥ä¸‹ã®ç”»åƒã¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯è¦–åŒ–ã®ä¾‹ã§ã™ã€‚

![ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¾‹](/images/opensearch-hnsw-hyperparameters-guide/example_metrics.png)

## åˆ¶é™äº‹é …ã¨ä»Šå¾Œã®èª²é¡Œ

æœ¬è¨˜äº‹ã§ã¯ã€recall ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¨ã„ã† 2 ã¤ã®ä¸»è¦ãªç›®çš„ã«å¯¾ã™ã‚‹ HNSW ã®æœ€é©åŒ–ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã—ãŸã€‚ãŸã ã—ã€HNSW ã‚°ãƒ©ãƒ•ã®ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«èª¿æ•´ã™ã‚‹ã«ã¯ã€`ef_construction` ã®ç•°ãªã‚‹å€¤ã‚’æ¢ç´¢ã™ã‚‹ã“ã¨ã§è¿½åŠ ã®çŸ¥è¦‹ãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

ç¾åœ¨ã®æ‰‹æ³•ã¯ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦åŒã˜è¨­å®šã‚»ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¾ã™ãŒã€ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã¯æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹æ€§ã‚’è€ƒæ…®ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šçš„ã‚’çµã£ãŸæ¨å¥¨ã‚’ä½œæˆã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€ç¾åœ¨ã®è¨­å®šã‚»ãƒƒãƒˆã¯ 15 ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚è¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚Šåºƒç¯„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€å­¦ç¿’ã•ã‚ŒãŸè¨­å®šã®æ±åŒ–æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã§ã—ã‚‡ã†ã€‚

ä»Šå¾Œã¯ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å‰Šæ¸›ã—ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€HNSW ã¨ä½µã›ã¦é‡å­åŒ–æ‰‹æ³•ã®æ¨å¥¨ã‚’å«ã‚ã‚‹ã‚ˆã†ç¯„å›²ã‚’æ‹¡å¤§ã™ã‚‹ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

## å‚è€ƒæ–‡çŒ®

1. Malkov, Yu A., and Dmitry A. Yashunin. "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs." IEEE transactions on pattern analysis and machine intelligence 42.4 (2018): 824-836.
2. Xu, Lin, Holger Hoos, and Kevin Leyton-Brown. "Hydra: Automatically configuring algorithms for portfolio-based selection." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 24. No. 1. 2010.
3. Winkelmolen, Fela, et al. "Practical and sample efficient zero-shot hpo." arXiv preprint arXiv:2007.13382 (2020).
4. Salinas, David, and Nick Erickson. "TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications." arXiv preprint arXiv:2311.02971 (2023).
5. Feurer, Matthias, and Frank Hutter. Hyperparameter optimization. Springer International Publishing, 2019.
6. Shahriari, Bobak, et al. "Taking the human out of the loop: A review of Bayesian optimization." Proceedings of the IEEE 104.1 (2015): 148-175.
