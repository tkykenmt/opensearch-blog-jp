---
title: "[翻訳] OpenSearch 3.3: AI 検索ソリューションのためのパフォーマンス革新"
emoji: "🚀"
type: "tech"
topics: ["opensearch", "ai", "vectorsearch", "performance", "search"]
published: true
publication_name: "opensearch"
published_at: 2025-12-08
---

:::message
本記事は [OpenSearch Project Blog](https://opensearch.org/blog/) に投稿された以下の記事を日本語に翻訳したものです。
:::

https://opensearch.org/blog/opensearch-3-3-performance-innovations-for-ai-search-solutions/

OpenSearch 3.3 は、パフォーマンスとイノベーションにおいて大きな前進を遂げています。3.x シリーズの最初のバージョンである OpenSearch 3.0 は 2025 年 5 月 6 日にリリースされ、検索パフォーマンスで 10 倍、ベクトルパフォーマンスで 2.5 倍の改善を実現しました。この基盤の上に、2025 年 10 月 14 日にリリースされた最新の 3.3 では、クエリ処理、インデックス作成、AI/ML 機能がさらに強化されています。

[OpenSearch 3.3](https://opensearch.org/blog/explore-opensearch-3-3/) では、標準的な Big5 ベンチマークスイートにおいて、[OpenSearch 1.3](https://opensearch.org/blog/launch-announcement-1-3-0/) と比較してクエリレイテンシが平均で約 11 倍改善されました。これらの改善は、新しい高性能 gRPC トランスポートレイヤー、よりスマートなクエリ実行戦略、集計用に最適化されたデータ構造、高度なベクトル処理の最適化など、多数の機能強化によるものです。

ベクトル検索のインデックス作成も OpenSearch 3.3 で大幅に改善され、OpenSearch 2.x バージョンと比較して、インデックス作成速度が約 9 倍高速化、ストレージが約 3 分の 1 に削減、ベクトル検索レイテンシが 55% 改善、マージ時間が 40% 短縮されました。GPU ベースのベクトルインデックスアクセラレーションにより、ベクトルエンジンはすべての圧縮およびベクトルデータ型のインデックスビルドをサポートするようになりました。

本記事では、OpenSearch 3.3 のパフォーマンス改善について、検索クエリレイテンシ、インデックス作成スループット、ベクトル検索やハイブリッド (セマンティックおよびテキスト) 検索などの新しいワークロードに焦点を当てて詳しく説明します。透明性と実用性を確保するため、Big5 ワークロードを使用したベンチマーク結果を提示します。最後に、すべての分野でパフォーマンスを継続的に向上させる計画を概説した 2026 年のロードマップを共有します。

## OpenSearch 3.3 のクエリパフォーマンス改善

OpenSearch 3.3 は、以下のグラフに示すように、2.x シリーズ全体で行われたパフォーマンス改善を拡張しています。OpenSearch 1.3 と比較して、主要なクエリタイプ全体で幾何平均クエリレイテンシが約 91% 削減され、約 11 倍高速なクエリに相当します。より最近の 2.x バージョンと比較しても、OpenSearch 3.3 は Big5 ベンチマークワークロードで OpenSearch 2.19 より約 33% の改善を示しています。

![OpenSearch レイテンシパフォーマンスグラフ](/images/opensearch-3-3-ai-search-performance/latency-performance.png)


### 主要なパフォーマンス改善

OpenSearch 3.3 では、3.0、3.1、3.2 で段階的に追加された改善とともに、いくつかの主要なパフォーマンス強化が導入されています。以下は、前回のパフォーマンスブログ記事以降に実装された最も影響力のある変更です。

- **新しい gRPC/protobuf トランスポートレイヤー**: OpenSearch 3.3 は、3.2 で一般提供された高性能 [gRPC トランスポートレイヤー](https://docs.opensearch.org/latest/api-reference/grpc-apis/index/)を使用して、効率的なノード間通信を実現しています。gRPC モジュールは、従来の REST/JSON API をコンパクトな Protocol Buffers に置き換え、ペイロードサイズと処理オーバーヘッドを削減します。このトランスポートは、バルクインデックス作成や k-NN (ベクトル) 検索などのパフォーマンスクリティカルな操作に使用されます。[OpenSearch Benchmark](https://github.com/opensearch-project/project-website/issues/3972#issuecomment-3439369642) での gRPC の初期ベンチマークでは、サービスレイテンシが 3〜4% 低下、クライアント側の処理時間が約 50% 削減、ベクトル検索ワークロードのスループットが約 20% 向上しています。OpenSearch Foundation のプレミアメンバーである Uber がこの機能に貢献したことに感謝します。

- **近似フレームワークによる最適化されたソート**: OpenSearch 3.2 は、`search_after` クエリの効率的なサポートを追加することで、ページネーションパフォーマンスの長年のギャップを解消しました。以前は、`search_after` を使用したディープページネーションは Lucene の非効率な線形スキャンにフォールバックしていました。OpenSearch 3.2 以降では、エンジンが `search_after` パラメータを範囲クエリに変換し、ソートされたデータに対して Lucene の最適化された BKD ツリートラバーサルを有効にします。結果は劇的で、Big5 データセットの時系列ベンチマークでは、`search_after` を使用したタイムスタンプフィールドのページネーション時に p90 クエリレイテンシが約 185 ms から約 8 ms に低下しました。この約 50 倍の改善により、ダッシュボードとディープページネーションクエリの応答性が大幅に向上します。

- **並行検索スレッドバランシング**: OpenSearch 3.0 は並行セグメント検索を導入し、コストのかかるクエリをインデックスセグメント間で並列実行できるようにしました。OpenSearch 3.3 では、スケジューラが改善され、スレッド間でより均等に作業を分散するようになりました。[新しいアプローチ](https://github.com/opensearch-project/OpenSearch/pull/18451)では、貪欲な負荷分散戦略を使用し、セグメントをサイズでソートして最も負荷の少ないスレッドグループに割り当て、スレッド間で作業を均等化します。実際には、この最適化により、並行性下での重い集計クエリのテールレイテンシがさらに [3〜5% 削減](https://github.com/opensearch-project/OpenSearch/pull/18451#issuecomment-3071232256)されます。

- **集計の最適化**:
  - **スターツリー集計**: 2.19 で最初に導入されたスターツリーインデックスは、分析クエリを高速化するためにメトリクスを事前集計します。OpenSearch 3.3 では、スターツリーサポートが[マルチターム集計](https://github.com/opensearch-project/OpenSearch/pull/19284)に拡張され、特定のログ分析シナリオで高速化を実現します。ベンチマークでは、マルチタームクエリにスターツリーインデックスを使用すると、高カーディナリティのキーワードまたは数値フィールドでレイテンシが約 3 倍削減され、複雑なクエリでは最大約 40 倍の改善が達成されることが示されています。
  - **レアターム集計**: OpenSearch 3.3 には、特定の条件下で[不要な処理をスキップ](https://github.com/opensearch-project/OpenSearch/pull/18978#issuecomment-3259633924)できるレアターム集計のコミュニティ貢献による最適化が含まれています。これにより、レアターム検索のクエリ時間が最大 50% 高速化されます。
  - **コンポジットターム集計**: OpenSearch 3.2 は、大規模なターム集合をページネーションするために使用されるコンポジット集計のパフォーマンスを改善しました。オブジェクトの再利用と[フィールドマッピング](https://github.com/opensearch-project/opensearch-benchmark-workloads/pull/645)の最適化により、コンポジットターム集計クエリで[約 5% の高速化](https://github.com/opensearch-project/OpenSearch/pull/9412)と、大規模集計のガベージコレクションオーバーヘッドの削減を実現しています。

- **日付ヒストグラム用スキップリスト**: 時系列集計は、新しい[スキップリストインデックス](https://github.com/opensearch-project/OpenSearch/issues/19384)により大幅なパフォーマンス向上を実現しました。スキップリストは、データ範囲に関する要約情報を格納する軽量な事前集計データ構造で、OpenSearch がクエリ中に無関係なセクションを素早くスキップできるようにします。OpenSearch 3.0 で[オプション機能](https://github.com/opensearch-project/OpenSearch/pull/18889)として最初に導入され、OpenSearch 3.2 ですべての数値フィールドに拡張され、バージョン 3.3 ではタイムスタンプフィールド (ログデータの `@timestamp` フィールドなど) でデフォルトで有効になりました。具体的なパフォーマンス改善には以下が含まれます。
  - フィルター付き時間別集計クエリで、スキップリストにより 96% のレイテンシ削減を達成
  - フィルターとメトリクス付き時間別集計クエリで、スキップリスト最適化により 46% のレイテンシ削減を達成

- **Apache Arrow によるストリーミング集計**: OpenSearch 3.3 は、Apache Arrow および Apache Arrow Flight テクノロジーを使用した集計処理の実験的なストリーミングアプローチを導入しています。各処理フェーズが完了するのを待ってから次を開始する代わりに、ストリーミング集計は部分的な結果が利用可能になり次第配信できます。これにより、メモリ使用量が削減され、複雑な集計の応答時間が改善されます。現在は実験的な機能フラグの下で利用可能で、このストリーミングアプローチは数値ターム集計とカーディナリティカウント集計をサポートしています。初期ベンチマークでは、非常に大規模なデータセットでの集計で最大 2 倍高速な応答時間が示されています。

- **検索エンジンコアの更新**: OpenSearch 3.3 は Apache Lucene 10.3 で動作し、OpenSearch 3.0 の Lucene 10.1 からアップグレードされています。この更新により、セグメントマージの強化、より効率的な圧縮、より高速なクエリ処理など、Lucene コミュニティからの多数の低レベルパフォーマンス改善とバグ修正がもたらされます。更新された Java ランタイム (OpenSearch 3.3.1 用の JDK 24) と組み合わせることで、これらの変更により OpenSearch は検索エンジンライブラリと JVM の両方の最新の機能強化を活用して、速度と効率を向上させることができます。

### クエリタイプ別のパフォーマンス改善

OpenSearch 3.3 は、以前のバージョンと比較してすべての一般的なクエリタイプでより高速なパフォーマンスを提供します。平均 (クエリ全体の幾何平均) では、OpenSearch 1.3 と比較して全体的なレイテンシが約 11 倍削減されています。OpenSearch 1.3 で 160 ms かかっていたクエリが、OpenSearch 3.3 では約 14 ms で完了するようになりました。パフォーマンス改善はすべての主要なクエリカテゴリにわたり、それぞれが大幅な改善を実現しています。

- **テキストクエリ**: テキスト検索操作は、1.3 と比較して OpenSearch 3.3 で約 89% 高速化されています。約 60 ms かかっていた典型的な全文検索クエリが、約 6.4 ms で完了するようになりました。これらの改善は、`match_only_text` フィールドタイプや強化されたクエリ実行などの最適化によるものです。

- **ターム集計**: 高カーディナリティのターム集計 (最も頻繁なターム上位 k 件の検索など) は、1.3 より約 87% 高速化され、テストではレイテンシが約 609 ms から約 79 ms に減少しました。2.x シリーズでこの分野に主要な最適化が導入され、3.x ではスターツリーやストリーミング集計などの機能でさらに効率的なパフォーマンスを実現しています。

- **日付ヒストグラム**: 時間ヒストグラムクエリ (イベントを時間または日ごとにバケット化するなど) は、OpenSearch 1.3 と比較して OpenSearch 3.3 で約 98% 低いレイテンシを示しています。ベンチマークでは、約 100 GB のログインデックスでの日付ヒストグラムが OpenSearch 1.3 の約 6 秒から OpenSearch 3.3 の約 0.09 秒に低下し、約 60 倍の高速化を実現しました。これらの改善は、スキップリストインデックスや並行セグメント処理などの機能によるもので、時間ベースの集計を非常に高速にしています。

- **範囲クエリ**: 数値範囲フィルタリング (2 つの数値間のすべての値を検索するなど) は、OpenSearch 3.3 で 1.3 より約 90% 高速に実行され、26 ms から約 2.6 ms に減少しました。この改善の多くは、新しい範囲クエリ近似フレームワークが一般提供された OpenSearch 3.0 で発生しました。インデックス化された境界とブロックスキップを使用することで、フレームワークは無関係なデータのスキャンを回避します。

- **ソートクエリ**: ソートされたクエリ (タイムスタンプまたは数値フィールドで結果を並べ替えるなど) は、OpenSearch 3.3 で 1.3 と比較して 75% 高速化され、応答時間が平均で約 17.7 ms から約 4.5 ms に減少しました。このパフォーマンス向上の大部分は、3.x での Lucene の最適化された時系列ソートの復元と、OpenSearch 3.2 で導入された `search_after` 最適化によるもので、ディープページネーションを大幅に高速化しています。

## AI/ML とベクトル検索の進歩

ベクトル検索やハイブリッドセマンティック検索などの AI/ML ワークロードは、OpenSearch 3.x の主要な焦点となっています。これらのアプリケーションは、高次元ベクトル計算のための集中的な処理を必要とし、大規模なデータセットを扱うことが多いため、パフォーマンス改善はコスト削減とスループット向上に特に価値があります。OpenSearch 3.3 はこの分野でいくつかの重要な進歩を実現しています。

- **高速な k-NN ベクトル検索 (FP16 最適化)**: OpenSearch の k-NN エンジン (Faiss などのライブラリを統合する k-NN プラグイン) は、OpenSearch 3.3 でメモリ最適化ベクトル検索を高速化する新しい最適化を導入しました。以前は、半精度 (FP16) ベクトルを使用する場合、JVM にネイティブ FP16 サポートがなかったため、距離計算前に FP32 への変換が必要で、メモリと計算のオーバーヘッドが追加されていました。OpenSearch 3.3 は、生の FP16 ベクトルデータをネイティブ C++ ルーチンに直接渡すことでこのボトルネックを解消し、FP32 変換なしで SIMD アクセラレーションされた距離計算を可能にしています。テストでは、これらの変更と楽観的な並列検索戦略を組み合わせることで、マルチセグメントベクトルクエリレイテンシで約 16.2% のパフォーマンス改善が得られ、リコール精度の低下はわずか約 2% でした。シングルセグメント (インメモリ) クエリでは、パフォーマンスが 55.8% 向上しました。

- **GPU パフォーマンス**: OpenSearch のベクトルエンジンは、コアパフォーマンス改善に加えて追加機能を獲得しました。GPU アクセラレーションされた k-NN インデックス作成 (OpenSearch 3.1 で一般提供) により、インデックスビルドが 9 倍高速化されました。GPU ベースのインデックスアクセラレーションは、バイトデータ型、バイナリデータ型、および複数の圧縮率 (2 倍、8 倍、16 倍、32 倍圧縮) を持つ量子化インデックスを、インメモリモードとオンディスクモードの両方でサポートしています。

## ハイブリッド検索

OpenSearch は、従来のキーワード検索とセマンティックベクトル検索を組み合わせたハイブリッド検索クエリをサポートしており、正確なキーワードとセマンティック類似性の両方を使用してドキュメントを検索できます。ハイブリッド検索機能は、より良いパフォーマンスのために継続的に最適化されています。OpenSearch 3.1 は、並行セグメント検索を最大限に活用するカスタムハイブリッドバルクスコアラーを導入し、以前の OpenSearch バージョンと比較してハイブリッドクエリで最大 65% 高速な応答と 3.5 倍高いスループットを実現しました。

OpenSearch 3.3 は、ハイブリッドクエリロジックを検索コレクターに直接統合できる新しい `QueryCollectorContextSpec` クラスでクエリ実行フェーズを合理化することで、この機能強化をさらに発展させています。これにより、テキストサブクエリを含むハイブリッド検索で追加の 20% のレイテンシ改善が実現し、テキストとベクトルの組み合わせクエリでは最大 5% の改善が得られます。

これらの機能強化により、ハイブリッド検索がより効率的になり、結果の関連性が正確なキーワードマッチとより広いセマンティック理解の両方に依存するエンタープライズ検索や質問応答などのユースケースに恩恵をもたらします。

## インデックス作成と取り込みパフォーマンス

OpenSearch 3.3 は、より高速なクエリだけでなく、データ取り込みとストレージ効率の大幅な改善も実現し、ストレージコストを削減しながらより多くのデータをより速くインデックス化できるようにしています。

- **[ストレージ節約のための派生ソース](https://opensearch.org/blog/save-up-to-2x-on-storage-with-derived-source/)**: OpenSearch 3.2 で導入された主要な機能強化は派生ソースインデックス作成で、OpenSearch 3.3 はこれを使用してインデックスサイズを大幅に削減しています。典型的な OpenSearch インデックスでは、各ドキュメントの元の JSON `_source` が、インデックス化されたタームと各フィールドのカラムナーデータ (doc values) とともに完全に格納され、かなりのストレージオーバーヘッドが発生します。派生ソースモードは、インデックス時に格納された `_source` を省略し、フィールドごとの値からオンデマンドで元のドキュメントを再構築することで、この重複を排除します。重要なのは、これにより完全な機能が維持されることです。ソースは動的に再生成できるため、検索、ハイライト、再インデックス、更新は通常どおり機能します。利点ははるかに小さいインデックスです。生の JSON の格納を回避することで、約 2 倍のストレージ節約を達成できます。ベンチマークでは、派生ソースを有効にすると、クエリ機能を失うことなく、大規模データセットでインデックスストレージが約 58% 削減されました。

- **より高速なインデックス作成スループット**: 派生ソースの追加の利点は、より高速な取り込みです。完全な `_source` を格納しないことで、OpenSearch はドキュメントごとの作業量が減少します (書き込みと圧縮するデータが少なくなります)。実際には、派生ソースを有効にすると最大約 18% のインデックス作成スループット改善が観察されました。マージ時間も 20〜48% 減少しました。エンジンがより小さなセグメントを書き込むためです。その結果、OpenSearch 3.3 は、より小さなインデックスによりコスト効率が高いだけでなく、インデックス作成パイプラインを効率的に保ちながらより高速になっています。

- **その他の取り込み機能強化**: OpenSearch は AI/ML ユースケース向けの取り込みパイプラインを継続的に合理化しています。例えば、3.x シリーズでは `text_embedding` 取り込みプロセッサに最適化が導入されました。変更されていない入力に対してエンベディング推論をスキップする設定可能なオプションで、冗長なモデル呼び出しを回避します。この単純な変更により、ドキュメントをエンベディングで強化するワークフローで取り込みレイテンシを最大 70% 削減できます。ドキュメントが変更なしで再インデックスされる場合、エンベディングは再計算されません。

## OpenSearch ベンチマークの改善

OpenSearch Benchmark は、2023 年 5 月の最初のメジャーリリース以来、OpenSearch の標準的なパフォーマンステストスイートとなっています。コミュニティによる [OpenSearch Benchmark 2.0](https://opensearch.org/blog/introducing-opensearch-benchmark-2-0/) のリリースは大きな進化を示しており、主要なテスト制限に対処し、複数の次元でベンチマーク機能を拡張する強化された機能を提供しています。

OpenSearch Benchmark 2.0 は、パフォーマンステストワークフローを改善する 5 つの革新的な機能を導入しています。[レッドラインテスト](https://opensearch.org/blog/redline-testing-now-available-in-opensearch-benchmark/)は、リアルタイム監視と自己調整負荷メカニズムを使用して、単一のテスト実行でクラスター容量の限界を特定します。合成データ生成により、組織は OpenSearch インデックスマッピングから大規模なプライバシー準拠データセットを作成でき、時系列データやベクトル分布などの複雑なワークロードをサポートします。ストリーミング取り込みにより、ローカルに格納された静的データを必要とせずに、単一のホストから毎日数テラバイトにスケールする高速での継続的なドキュメント取り込みが可能になります。

公式ワークロードコレクションも 2 つの注目すべき追加で強化されました。Big5 ワークロードに Piped Processing Language (PPL) 形式のサポートが含まれるようになり、ClickBench データセットに基づく新しい ClickHouse ワークロードには Yandex.Metrica ウェブ分析データが含まれています。

## 2026 年のロードマップ

[公開ロードマップ](https://github.com/orgs/opensearch-project/projects/206/views/20)に基づき、2026 年は OpenSearch に大きなアーキテクチャの進化をもたらし、ストリーミングクエリ処理、コンポーザブルエンジン設計、強化された gRPC API、高度なベクトル検索機能にわたる主要なイニシアチブが予定されています。

### ストリーミングクエリアーキテクチャ

今後の方向性は、[この提案](https://github.com/opensearch-project/OpenSearch/issues/16679)で概説されているように、マイルストーン 3.5 でデフォルトでストリーミング集計を有効にすることに焦点を当てています。この作業は、クエリプランニング、リクエストキャッシング、トップ N 計算、仮想スレッド、ベンチマークなどの分野で残っているギャップを解消します。

並行して、ロードマップはメモリ効率を改善するためのファーストクラスコンポーネントとしてのカラムナー形式のサポート拡張、より高い精度のための分散マルチレベルリデュース、高コストクエリのためのストリーミング検索、より広いプラグイン採用を継続的に推進しています。

### OpenSearch コンポーザブルクエリエンジンアーキテクチャ

OpenSearch は、密結合された Lucene 基盤からプラガブルな実行エンジンとファイル形式をサポートするコンポーザブルクエリエンジンへと進化するための主要なアーキテクチャ変革を提案しています。これは、大規模データセットでの集計におけるメモリとストレージのボトルネック、クエリ言語 (クエリ DSL、SQL、PPL) 間での重複ロジックなど、現在の制限に対処します。ビジョンは、言語フロントエンドが Substrait 中間表現を使用して共通の論理プランに変換し、中央プランニングレイヤーがコストベースの最適化を実行し、プラガブルな実行エンジン (現在 DataFusion と Velox を評価中) が論理プランを受け入れて Apache Arrow レスポンスを生成することを中心としています。

### gRPC ベースの検索 API

REST API と gRPC のギャップを解消する作業が継続されており、gRPC の利点を示す追加のベンチマークが含まれています。この取り組みは、`date_histogram`、`terms`、`cardinality`、`stats`、`percentiles`、地理的集計など 50 以上の集計タイプをカバーしており、protobuf スキーマに追加され、gRPC トランスポートモジュールに実装されています。

### コア検索エンジンパフォーマンスの最適化

OpenSearch は、クエリと集計のパフォーマンスを高速化する低レベルエンジンの改善を継続的に探求しています。この分野の主要なイニシアチブには以下が含まれます。

- **[スキップリストベースの最適化](https://github.com/opensearch-project/OpenSearch/issues/19384)**: OpenSearch 3.4 は、範囲および自動日付ヒストグラムを持つサブ集計として機能するように日付ヒストグラムサポートを拡張します。次のステップは、複数の所有バケット序数を追跡しながらサブ集計を許可するようにロジックを変更することです。このロジックは `min` および `max` 集計に組み込まれ、`DocValuesSkipper` からの事前集計データを使用してそれらを高速化します。

- **[Lucene バルクコレクション API を使用した集計の高速化](https://github.com/opensearch-project/OpenSearch/issues/19324)**: Lucene の新しいバルクコレクション API を使用して集計パフォーマンスを改善することを探求しています。Lucene 10.3 の `LeafCollector#collectRange` や、`NumericDocValues#longValues` や `DocIdStream#intoArray` などの今後の 10.4 API などの機能を使用します。

- **[セグメント内並行検索](https://github.com/opensearch-project/OpenSearch/issues/18852)**: Lucene 10 で導入されたセグメント内並行検索の有効化に取り組んでいます。これは、複数のスレッドにわたる並行処理のために個々のセグメントをドキュメント ID で分割します。これにより、既存の並行セグメント検索機能を超えた、より細かい粒度の並列処理が提供されます。

### ベクトル検索

OpenSearch は、高次元データのより効率的でスケーラブルで拡張可能なワークフローをサポートするために、ベクトル検索機能を継続的に強化しています。主要なイニシアチブには以下が含まれます。

- **拡張範囲のための Faiss スカラー量子化器への BFloat16 サポートの追加**: [Faiss スカラー量子化器への BFloat16 サポートの追加](https://github.com/opensearch-project/k-NN/issues/2510)により、k-NN プラグインは既存の FP16 実装の範囲制限を克服できます。FP16 は入力ベクトルを [-65,504, 65,504] に制限し、50% のメモリ削減と FP32 に匹敵するパフォーマンスにもかかわらず、デフォルトのデータ型になることを妨げています。BFloat16 (SQbf16) は、精度を犠牲にして 50% のメモリ節約を維持しながら、FP32 と同じ拡張範囲 (約 ±3.4 × 10³⁸) を提供します。

- **メモリ最適化検索をデフォルトに**: OpenSearch 3.3 では、k-NN プラグインは Lucene ライブラリの HNSW グラフトラバーサルアルゴリズムと C++ バルク SIMD ベースの距離計算を組み合わせることで、メモリ最適化検索で大幅な改善を達成しました。将来の最適化には、テールレイテンシを削減するためのメモリ最適化検索インデックスの[ウォームアップ機能](https://github.com/opensearch-project/k-NN/issues/2939)の追加と、メモリフットプリントを 50% 削減するための[メモリ最適化検索での FP16 のデフォルト化](https://github.com/opensearch-project/k-NN/issues/2924)が含まれます。

- **ディスクベースベクトル検索の強化**: OpenSearch 2.17 では、k-NN プラグインが[ディスクベースベクトル検索サポート](https://opensearch.org/blog/Reduce-Cost-with-Disk-based-Vector-Search/)を導入し、低メモリ環境での検索実行を可能にしました。ディスクベースベクトル検索のバージョン 2 は、Bi-partite Graph Partitioning (BPGP) や Gorder Priority Queue (Gorder-PQ) などの技術を使用してディスク上のベクトルを並べ替え、ディスクアクセスごとに取得されるベクトル数を最大化することで、ディスク読み取りを削減します。

- **インデックス作成と検索パフォーマンスの高速化**: OpenSearch は、x86 および ARM インスタンスでの検索パフォーマンスを改善するために、[avx512_fp16](https://github.com/facebookresearch/faiss/pull/4225)、BFloat16、ARM SVE などの新しい SIMD 命令によるハードウェアアクセラレーションを継続的に使用しています。GPU を使用したリモートインデックスビルドでは、GPU マシン間の[インデックスファイル転送](https://github.com/opensearch-project/remote-vector-index-builder/issues/94)の削減が計画されており、GPU ベースのインデックスビルドが 2 倍改善されると予想されています。

- **OpenSearch ベクトルエンジンの拡張性向上**: [ベクトル検索インターフェースを k-NN プラグインから OpenSearch コアに移動](https://github.com/opensearch-project/OpenSearch/issues/20050)することで、成長するベクトル検索環境における拡張性の課題とプラグイン依存関係に対処します。現在、k-NN プラグイン (Lucene と Faiss をサポート) と新しい JVector プラグインは独自の実装で動作していますが、標準化されたインターフェースがありません。この提案は、共通のベクトル検索インターフェースを OpenSearch コアに昇格させ、新しいベクトルエンジンのより良い拡張性を可能にし、Neural Search プラグインが k-NN プラグインに直接依存するハード依存関係を排除し、ユーザーが Neural Search プラグインで任意のベクトルプラグイン (k-NN、JVector、または将来のエンジン) を選択できるようにします。

## 付録: ベンチマーク方法論

すべてのパフォーマンス比較は、OpenSearch Benchmark ツールと Big5 ワークロードに基づく再現可能なプロセスを使用して実施されました。ベンチマークは、マッチクエリ、ターム集計、範囲フィルター、日付ヒストグラム、ソートクエリをカバーしました。データセット (約 100 GB、1 億 1600 万ドキュメント) は、時系列および e コマースのユースケースを反映しています。

**環境**: テストは、c5.2xlarge Amazon EC2 インスタンス (8 vCPU、16 GB RAM、8 GB JVM ヒープ) を使用した単一ノード OpenSearch クラスターで実行されました。特に記載がない限り、デフォルト設定が使用されました。インデックスには 1 つのプライマリシャードがあり、マルチシャードの変動を避けるためにレプリカはありませんでした。ドキュメントは時系列ワークロードをシミュレートするために時系列順に取り込まれました。

**インデックス設定**: Lucene の `LogByteSizeMergePolicy` を使用し、明示的なインデックスソートは有効にしませんでした。一部のテストでは、公平な比較を確保するためにセグメント数を正規化するためにフォースマージが適用されました (例: 2.19 と 3.3 の両方で 10 セグメント)。

**実行**: 各操作は複数回繰り返されました。ウォームアップ実行を破棄し、次の 3 回の実行を平均しました。レイテンシメトリクスには p50、p90、p99 が含まれ、スループットも記録されました。OpenSearch Benchmark は、各操作タイプの正確なクエリレイテンシを記録するためにスループットスロットルモードで実行されました。

**ソフトウェア**: 比較には OpenSearch 2.19.1 (Java 17) と 3.3.1-beta (Java 24、Lucene 10.3.1) を使用しました。デフォルトのプラグインのみが有効でした。ベクトルベンチマークでは、k-NN プラグインを使用した Faiss + HNSW を使用し、リコールはブルートフォース結果に対して測定されました。

**メトリクス**: *Big5 中央値レイテンシ*は 5 つのコアクエリタイプの単純平均です。*集計レイテンシ*は幾何平均で、全体的な比較に使用されます。高速化係数は、記載されている場合、OpenSearch 1.3 を基準として報告されています。
