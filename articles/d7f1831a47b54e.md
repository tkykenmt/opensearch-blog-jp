---
title: "OpenSearch ã§æ—¥æœ¬èªž Sparse search ã‚’å‹•ã‹ã—ã¦ã¿ã‚‹"
emoji: "ðŸ“š"
type: "tech"
topics:
  - "æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³"
  - "å…¨æ–‡æ¤œç´¢"
  - "opensearch"
  - "ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢"
published: true
published_at: "2024-12-27 23:00"
---

æœ¬æŠ•ç¨¿ã¯ä»¥ä¸‹ã‚¢ãƒ‰ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã¨ãƒªãƒ³ã‚¯ã—ã¦ã„ã¾ã™ã€‚

- AWS Analytics Advent Calendar 2024 (12/25 åˆ†) 
- æƒ…å ±æ¤œç´¢ãƒ»æ¤œç´¢æŠ€è¡“ Advent Calendar 2024 (ã‚·ãƒªãƒ¼ã‚º2 12/25 åˆ†)

https://qiita.com/advent-calendar/2024/aws-analytics

https://qiita.com/advent-calendar/2024/search


## OpenSearch ã«ãŠã‘ã‚‹ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®æ¦‚è¦
OpenSearch ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®å…¨æ–‡æ¤œç´¢ãƒ»åˆ†æžã‚¹ã‚¤ãƒ¼ãƒˆã§ã™ã€‚OpenSearch ã¯å…¨æ–‡æ¤œç´¢ã«åŠ ãˆã¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®æ©Ÿèƒ½ã‚‚å‚™ãˆã¦ã„ã¾ã™ã€‚

ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯å¾“æ¥ã®å…¨æ–‡æ¤œç´¢ã«ã‚ˆã‚‹æ¤œç´¢ãŒè‹¦æ‰‹ã¨ã™ã‚‹ã‚ã„ã¾ã„ãªã‚¯ã‚¨ãƒªã®å‡¦ç†ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€ã‚ã„ã¾ã„æ¤œç´¢ã‚„ LLM ã¨çµ„ã¿åˆã‚ã›ãŸæ¤œç´¢æ¤œç´¢æ‹¡å¼µç”Ÿæˆ(RAG) ã«ä»£è¡¨ã•ã‚Œã‚‹æ–‡æ›¸æ¤œç´¢ãƒ»ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ã§å¹…åºƒãæ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

ä¸€èˆ¬çš„ã«ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨å‘¼ã°ã‚Œã¦ã„ã‚‹ã®ã¯ã€N æ¬¡å…ƒã®æ•°å€¤é…åˆ—ã‹ã‚‰ãªã‚‹å¯†ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ã£ãŸæ¤œç´¢ã§ã™ã€‚å¯†ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§ã¯ã€ã‚¯ã‚¨ãƒªã¨æ¤œç´¢å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã¯ N æ¬¡å…ƒã®æ•°å€¤é…åˆ—ã¨ã—ã¦æ‰±ã‚ã‚Œã€ãã‚Œã‚‰ã®è·é›¢ã‚„è§’åº¦ã®å·®ç•°ãŒé¡žä¼¼åº¦ã¨ã—ã¦è¡¨ã•ã‚Œã¾ã™ã€‚è·é›¢ã‚„è§’åº¦ãŒè¿‘ã„ã»ã©é¡žä¼¼åº¦ãŒé«˜ã„ã¨ã¿ãªã•ã‚Œã‚‹ã‚ã‘ã§ã™ã€‚

å…¨æ–‡æ¤œç´¢ã¯ã‚¯ã‚¨ãƒªã¨æ¤œç´¢å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿é–“ã§åŽ³å¯†ãªãƒžãƒƒãƒãƒ³ã‚°ãŒè¦æ±‚ã•ã‚Œã‚‹ä¸€æ–¹ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¯"æ„å‘³çš„ã«è¿‘ã„" æ–‡æ›¸ã‚’å–å¾—ã™ã‚‹éš›ã«æœ‰ç”¨ã§ã‚ã‚‹ãŸã‚ã€ã†ã¾ãä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ã§å¹…åºƒã„æ¤œç´¢è¦ä»¶ã‚’é”æˆã§ãã¾ã™ã€‚

### å¾“æ¥åž‹ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®èª²é¡Œ
ã‚¯ã‚¨ãƒªã¨ãƒ‡ãƒ¼ã‚¿ã®é¡žä¼¼åº¦ã‚’åˆ¤å®šã™ã‚‹ãŸã‚ã«ã¯ã€ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã™ã‚‹éƒ½åº¦ã€ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«ã¨æ¤œç´¢å¯¾è±¡ã®ãƒ™ã‚¯ãƒˆãƒ«é–“ã®è·é›¢è¨ˆç®—ãŒå¿…è¦ã¨ãªã‚Šã¾ã™ã€‚å°è¦æ¨¡ãªãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ¤œç´¢ã§ã‚ã‚Œã°ã€ã‚¯ã‚¨ãƒªã¨å…¨ãƒ™ã‚¯ãƒˆãƒ«é–“ã®è·é›¢ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã‚‚ç¾å®Ÿçš„ãªé¸æŠžè‚¢ã¨ãªã‚‹ã®ã§ã™ãŒã€æ¤œç´¢å¯¾è±¡ã®ãƒ™ã‚¯ãƒˆãƒ«æ•°ãŒå„„å˜ä½ã«ä¸Šã‚‹ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®é¢ã§å®Ÿç”¨çš„ã§ã¯ãªããªã‚Šã¾ã™ã€‚ã“ã®å ´åˆã€Approximate kNN ã¨å‘¼ã°ã‚Œã‚‹ã€ç²¾åº¦ã‚’å°‘ã€…çŠ ç‰²ã«ã™ã‚‹ä»£ã‚ã‚Šã«æŽ¢æŸ»å›žæ•°ã‚’å‰Šæ¸›ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå–ã‚‰ã‚Œã¾ã™ã€‚

é«˜é€Ÿãªå¯†ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿç¾ã™ã‚‹ã†ãˆã§ã¯ã€æ¤œç´¢å¯¾è±¡ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒãƒ¡ãƒ¢ãƒªä¸Šã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ãŸã‚ã€è¦æ¨¡ãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦è¦æ±‚ãƒ¡ãƒ¢ãƒªã‚‚æ‹¡å¤§ã—ã€ã‚³ã‚¹ãƒˆãŒå¢—å¤§ã—ã¾ã™ã€‚

ãƒ™ã‚¯ãƒˆãƒ«ã®é‡å­åŒ–ã‚„ Disk-ANN ã¨å‘¼ã°ã‚Œã‚‹ä»•çµ„ã¿ã§è¦æ±‚ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã™ãŒã€ä¾ç„¶ã¨ã—ã¦ãƒ¡ãƒ¢ãƒªä¸Šã«ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹å¿…è¦æ€§ã¯æ®‹ã‚Šã¾ã™ã€‚

### ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢
OpenSearch ã§ã¯ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 2.11 ã‚ˆã‚Š Sparse neural search æ©Ÿèƒ½ã«ã‚ˆã‚‹ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚

ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸæ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã§ã¯ãªãã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã«å¯¾ã—ã¦æ¤œç´¢ã‚’è¡Œã„ã¾ã™ã€‚

ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®éŽç¨‹ã§ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¯æ„å‘³çš„ã«é¡žä¼¼ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®èªžå½™(WordPiece)ã«ã¯ã€æœ€ã‚‚ä¸€èˆ¬çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹å˜èªžã«åŠ ãˆã¦ã€æ§˜ã€…ãªæ™‚åˆ¶ã®èªžå°¾(ä¾‹: -edã‚„-ing)ã‚„æŽ¥å°¾è¾ž(ä¾‹: -ateã‚„-ion)ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®èªžå½™ã¯ã€å„æ–‡æ›¸ãŒã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¡¨ç¾ã•ã‚Œã‚‹æ„å‘³ç©ºé–“ã¨ã—ã¦è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ä»¥ä¸‹ã¯å¯†ãƒ™ã‚¯ãƒˆãƒ«ã¨ Sparse search ã«ãŠã‘ã‚‹ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®é•ã„ã‚’è¡¨ã—ãŸã‚‚ã®ã§ã™ã€‚

![](https://storage.googleapis.com/zenn-user-upload/526cd23798c1-20241223.png)

![](https://storage.googleapis.com/zenn-user-upload/6f6f4f83d1a5-20241223.png)

å‡ºå…¸:
https://opensearch.org/blog/improving-document-retrieval-with-sparse-semantic-encoders

OpenSearch ã§ã¯ã€ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã¯è»¢ç½®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã®ãŸã‚ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ™ã‚¯ãƒˆãƒ«ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã¨é‡ã¿ä»˜ã‘ã®ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ãƒªã‚¹ãƒˆã¨ã—ã¦ã€rank_features ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«æ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å¾“æ¥ã®å…¨æ–‡æ¤œç´¢ã«è¿‘ã„å½¢ã§ã‚ã„ã¾ã„æ¤œç´¢ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚

## æ—¥æœ¬èªžã«ã‚ˆã‚‹ Sparse search å®Ÿè£…ã®æµã‚Œ

Sparse search ã«ãŠã„ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€OpenSearch Project ã§ã¯ã„ãã¤ã‹ã®[ãƒ¢ãƒ‡ãƒ«](https://opensearch.org/blog/neural-sparse-v2-models/)ã‚’ä½œæˆãƒ»å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚ã—ã‹ã—ãªãŒã‚‰ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯è‹±èªžã®ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€æ—¥æœ¬èªžã‚’é©åˆ‡ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚

ä»Šå›žã¯æ—¥æœ¬èªžã«å¯¾å¿œã—ãŸã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ [japanese-splade-v2](https://huggingface.co/hotchpotch/japanese-splade-v2) ã‚’ä½¿ã‚ã›ã¦ã„ãŸã ãã€æ—¥æœ¬èªžã«ã‚ˆã‚‹ Sparse search ã‚’ OpenSearch ä¸Šã§å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ãã¾ã™ã€‚

æœ¬ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã‚„ã€æœ¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ¼ã‚¹ã¨ãªã£ã¦ã„ã‚‹ SPLADE ã«ã¤ã„ã¦ã®è§£èª¬ã«ã¤ã„ã¦ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®è£½ä½œè€…ã§ã‚ã‚‹ã‚»ã‚³ãƒ³ã•ã‚“ã«ã‚ˆã‚‹[ãƒªãƒªãƒ¼ã‚¹](https://secon.dev/entry/2024/12/19/100000-japanese-splade-v2-release/)ã‚’ã”è¦§ãã ã•ã„ã€‚

### æº–å‚™ä½œæ¥­
ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ã€ Amazon OpenSearch Service ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ç”¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚è¨˜äº‹åŸ·ç­†æ™‚ç‚¹ã§ã¯ OpenSearch Serverless ã§ã¯ Sparse search ã‚’åˆ©ç”¨ã§ããªã„ãŸã‚ã€ OpenSearch 
2.17 ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã€OpenSearch ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

ã¾ãŸã€ä½œæ¥­ã¯ Jupyter Notebook ã‚‚ã—ãã¯ JupyterLab ä¸Šã§è¡Œã„ã¾ã™ã€‚äº‹å‰ã« Amazon SageMaker Notebook ã‚‚ã—ãã¯ Amazon SageMaker Studio ãªã©ã®ç’°å¢ƒã‚’ç”¨æ„ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

ä»¥é™ã®ä½œæ¥­ã¯å…¨ã¦ JupyterLab ä¸Šã§å®Ÿè¡Œã—ãŸã‚‚ã®ã§ã™ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤
JupyterLab ä¸Šã§å®Ÿè¡Œã—ã¦ã„ãã¾ã™ã€‚

#### äº‹å‰ä½œæ¥­
ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®ã‚»ãƒƒãƒˆã€å¤‰æ•°ã®ã‚»ãƒƒãƒˆãªã©ã‚’å®Ÿæ–½ã—ã¦ã„ã¾ã™ã€‚ã“ã®éƒ¨åˆ†ã¯ãã‚Œã»ã©é‡è¦ãªå†…å®¹ã§ã¯ãªã„ã®ã§ã€ä¸€ã¤ä¸€ã¤ã®å‡¦ç†ã‚’è©³ã—ãè¦‹ã¦ã„ãå¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

**ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
```
!sudo apt-get update -y
!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
!sudo apt-get install git-lfs git -y
!git lfs install
```

**ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ**
```
import boto3
import json
from datetime import datetime, timedelta
import time
from functools import lru_cache
import pandas as pd
import numpy as np
import sagemaker
from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri
```

**ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®ã‚»ãƒƒãƒˆ**
```
def get_huggingface_tei_image_uri(instance_type, region):
  key = "huggingface-tei" if instance_type.startswith("ml.g") or instance_type.startswith("ml.p") else "huggingface-tei-cpu"
  return get_huggingface_llm_image_uri(key, version="1.2.3", region=region)

def search_endpoint(model_id, region):
    sagemaker_client = boto3.client("sagemaker", region_name=region)
    response = sagemaker_client.search(
        Resource="Endpoint",
        SearchExpression={
            "Filters": [
                {
                    "Name": "EndpointName",
                    "Operator": "Contains",
                    "Value": model_id
                },
            ],
        },
        SortBy="LastModifiedTime",
        SortOrder="Descending",
        MaxResults=1,
    )
    return response["Results"]

@lru_cache(maxsize=None)
def list_instance_quotas_for_realtime_inference(instance_family, region):
    service_quotas_client = boto3.client("service-quotas", region_name=region)
    quotas = []
    paginator = service_quotas_client.get_paginator('list_service_quotas')
    page_iterator = paginator.paginate(ServiceCode='sagemaker',
                                       PaginationConfig={'MaxResults': 100})
    for page in page_iterator:
        for quota in page['Quotas']:
            if quota["QuotaName"].endswith("endpoint usage") and quota["Value"] > 0 and quota["QuotaName"].startswith("ml."+instance_family):
                quotas.append(quota)
    return quotas

def list_instance_usages_from_quotas(quotas, region):
    cloudwatch_client = boto3.client("cloudwatch", region_name=region)
    end_time = datetime.utcnow()
    end_time = end_time.replace(minute=end_time.minute - (end_time.minute % 5),
                               second=0,
                               microsecond=0)  - timedelta(minutes=5)
    start_time = end_time - timedelta(hours=1)
    metric_data_queries = []
    for i, quota in enumerate(quotas):
        metric = quota['UsageMetric']
        dimensions = []
        for key, value in metric['MetricDimensions'].items():
            dimensions.append({
                'Name': key,
                'Value': value
            })
        metric_data_queries.append({
            'Id': f'usage_{i}',
            'MetricStat': {
                'Metric': {
                    'Namespace': metric['MetricNamespace'],
                    'MetricName': metric['MetricName'],
                    'Dimensions': dimensions
                },
                'Period': 300,  # 5åˆ†é–“éš”
                'Stat': metric['MetricStatisticRecommendation']
            }
        })
    payload = {
        'MetricDataQueries': metric_data_queries,
        'StartTime': start_time,
        'EndTime': end_time
    }
    usages = cloudwatch_client.get_metric_data(**payload)
    return usages

def list_instance_usages(region):
    sagemaker_client = boto3.client("sagemaker", region_name=region)
    response = sagemaker_client.list_endpoints(
    )
    instances = []
    for endpoint in response["Endpoints"]:
        response = sagemaker_client.describe_endpoint(EndpointName=endpoint["EndpointName"])
        response = sagemaker_client.describe_endpoint_config(EndpointConfigName=response["EndpointConfigName"])
        instances.append(response["ProductionVariants"][0]["InstanceType"])
    values, counts = np.unique(instances, return_counts=True)    
    return values,counts

def list_instance_attributes_realtime_inference(instance_family, region):
    pricing = boto3.client("pricing", region_name="us-east-1")
    instance_types = []
    paginator = pricing.get_paginator("get_products")
    page_iterator = paginator.paginate(
        ServiceCode="AmazonSageMaker",
        Filters=[
            {
                "Type": "TERM_MATCH",
                "Field": "productFamily",
                "Value": "ML Instance"
            },
            {
                "Type": "TERM_MATCH",
                "Field": "regionCode",
                "Value": region
            },
            {
                "Type": "TERM_MATCH",
                "Field": "platoinstancetype",
                "Value": "Hosting"
            },
            {
                "Type": "TERM_MATCH",
                "Field": "platoinstancename",
                "Value": instance_family
            },
        ],
    )
    products = []
    for page in page_iterator:
        for product in page["PriceList"]:
            products.append(json.loads(product)["product"]["attributes"])
    return products

def list_available_instance_types_for_realtime_inference(instance_family, region):
    quotas = list_instance_quotas_for_realtime_inference(instance_family=instance_family, region=region)
    quotas_df = pd.json_normalize(quotas).loc[:,["QuotaName","Value"]]
    quotas_df["InstanceType"] = quotas_df["QuotaName"].str.removesuffix(" for endpoint usage")
    quotas_df = quotas_df.drop(columns=["QuotaName"]).rename(columns={"Value":"Limit"})
    quotas_df["Limit"] = quotas_df["Limit"].astype(int)
    
    usage_values,usage_counts = list_instance_usages(region=region)
    usages_df = pd.DataFrame({"InstanceType": usage_values, "Usage": usage_counts})

    attributes = list_instance_attributes_realtime_inference(instance_family=instance_family, region=region)
    attributes_df = pd.json_normalize(attributes)
    attributes_df = attributes_df.loc[:,["instanceName","vCpu"]].rename(columns={"instanceName": "InstanceType"})
    
    merged_df = pd.merge(pd.merge(quotas_df, usages_df, how="left", on="InstanceType"), attributes_df, on='InstanceType').fillna(value=0)

    filtered_df = merged_df.query("Usage<Limit")
    instance_types = filtered_df.sort_values("vCpu", ascending=True, ignore_index=True).InstanceType

    return instance_types.array
```

**å¤‰æ•°ç­‰ã®ã‚»ãƒƒãƒˆ**
```
sagemaker_region = boto3.Session().region_name

default_instance_family_gpu = "g5"
default_instance_family_cpu = "m5"

py_version='py310',
transformers_version="4.37.0", # transformers version used
pytorch_version="2.1.0", # pytorch version used
```

#### ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤
Huggingface ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€Amazon SageMaker ã®æŽ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

**ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**
```
hf_model_id_sparse_embedding = "hotchpotch/japanese-splade-v2"
# hf_model_id_sparse_embedding = "hotchpotch/japanese-splade-base-v1"
model_id_sparse_embedding = hf_model_id_sparse_embedding.lower().replace("/", "-")
role = sagemaker.get_execution_role()
session = sagemaker.Session()
default_bucket = session.default_bucket()
s3_location=f"s3://{default_bucket}/custom_inference/{model_id_sparse_embedding}/model.tar.gz"
```

```
%pushd
%mkdir -p ./models
%cd ./models
!git clone https://huggingface.co/$hf_model_id_sparse_embedding
```

#### ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨ãƒ‡ãƒ—ãƒ­ã‚¤
ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’å…ƒã«ã€Amazon SageMaker ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã€ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã€ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

```
%mkdir -p ./code
```

Amazon SageMaker ã«ãŠã‘ã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æŽ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã¯ã€model_fn ã§ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã€predict_fn ã§æŽ¨è«–å®Ÿè¡Œã‚’è¡Œã„ã¾ã™ã€‚

model_fn å†…ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã«ã‚»ã‚³ãƒ³ã•ã‚“ãŒä½œæˆã•ã‚ŒãŸ [yasem](https://github.com/hotchpotch/yasem) ã¨å‘¼ã°ã‚Œã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨å®Ÿè¡Œã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚

```
%%writefile ./code/inference.py
from yasem import SpladeEmbedder

def model_fn(model_dir, context=None):
    model = SpladeEmbedder(model_dir)
    return model

def predict_fn(input_data, model):
    text_docs = input_data["inputs"]
    embeddings = model.encode(text_docs)
    token_values = model.get_token_values(embeddings)
    results = {
        "response": [token_values]
    }
    return results
```

inference.py ã‹ã‚‰å‚ç…§ã—ã¦ã„ã‚‹ yasem ãŠã‚ˆã³æœ¬ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã«å¿…è¦ãª fugashi, unidic_lite ã‚’å°Žå…¥ã™ã‚‹ãŸã‚ã« requirements.txt ã‚’ä½œæˆã—ã¾ã™

```
%%writefile ./code/requirements.txt
yasem==0.3.2 
fugashi
unidic_lite
```

ä½œæˆã—ãŸ inference.py ã¨ requirements.txt ã‚’ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã«ã‚³ãƒ”ãƒ¼ã—ã€ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’ä½œæˆã—ã¦ Amazon S3 ãƒã‚±ãƒƒãƒˆã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™

```
model_dir_sparse_embedding = hf_model_id_sparse_embedding.split("/")[-1]
```

```
!cp -rf ./code/ ./$model_dir_sparse_embedding/code/
%cd $model_dir_sparse_embedding
!rm -f model.tar.gz
!tar zcvf model.tar.gz *
!aws s3 cp model.tar.gz $s3_location
```

ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å…ƒã«ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æ§‹ç¯‰ã—ã¾ã™

```
available_instance_types = list_available_instance_types_for_realtime_inference(instance_family=default_instance_family_gpu, region=sagemaker_region)
instance_type = available_instance_types[0]
print(f"start deploy {hf_model_id_sparse_embedding} on {instance_type}")

role = sagemaker.get_execution_role()
huggingface_model_sparse_embedding = HuggingFaceModel(
   model_data=s3_location,       # path to your model and script
   entry_point='inference.py',
   source_dir='code',
   role=role,                    # iam role with permissions to create an Endpoint
   py_version='py310',
   transformers_version="4.37.0", # transformers version used
   pytorch_version="2.1.0", # pytorch version used
)
huggingface_model_sparse_embedding.deploy(
    endpoint_name=sagemaker.utils.name_from_base(model_id_sparse_embedding),
    initial_instance_count=1,
    instance_type=instance_type
)
```

ä½œæ¥­å¾Œã¯å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æˆ»ã£ã¦ãŠãã¾ã—ã‚‡ã†ã€‚

```
%popd
```

#### æŽ¨è«–ã®ãƒ†ã‚¹ãƒˆ
æŽ¨è«–ã®ãƒ†ã‚¹ãƒˆã‚’ã—ã¦ã„ãã¾ã™ã€‚"è»Šã®ç‡ƒè²»ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã¯ï¼Ÿ" ã¨å‘¼ã°ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚

```
model_endpoint_name_sparse_embedding = search_endpoint(model_id_sparse_embedding,sagemaker_region)[0]["Endpoint"]["EndpointName"]
model_endpoint_url_sparse_embedding = f"https://runtime.sagemaker.{sagemaker_region}.amazonaws.com/endpoints/{model_endpoint_name_sparse_embedding}/invocations"

print("sparse embedding endpoint name: " + model_endpoint_name_sparse_embedding)
print("sparse embedding model endpoint url: " + model_endpoint_url_sparse_embedding)

payload = {
    "inputs": [
        "è»Šã®ç‡ƒè²»ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã¯ï¼Ÿ"
    ]
}
body = bytes(json.dumps(payload), 'utf-8')

sagemaker_runtime_client = boto3.client("sagemaker-runtime",region_name=sagemaker_region)
response = sagemaker_runtime_client.invoke_endpoint(
    EndpointName=model_endpoint_name_sparse_embedding,
    ContentType="application/json",
    Accept="application/json",
    Body=body
)

result = eval(response['Body'].read().decode('utf-8'))
result
```

çµæžœã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã‚Œã§ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚‹æŽ¨è«–ãŒå¯èƒ½ã¨ãªã£ãŸãŸã‚ã€OpenSearch ã¨ã®é€£æºã«é€²ã¿ã¾ã™ã€‚

```
{'response': [[{'ç‡ƒè²»': 1.130859375,
    'æ–¹æ³•': 1.0693359375,
    'è»Š': 1.05078125,
    'é«˜ã‚ã‚‹': 0.67041015625,
    'å‘ä¸Š': 0.55615234375,
    'å¢—åŠ ': 0.5244140625,
    'éƒ½å¸‚': 0.443115234375,
    'ã‚¬ã‚½ãƒªãƒ³': 0.31982421875,
    'æ”¹å–„': 0.297607421875,
    'ã“ã¨': 0.287353515625,
    'ã›': 0.21533203125,
    'ä¸Šæ˜‡': 0.18896484375,
    'ã¹ã': 0.1451416015625,
    'ã™ã‚‹': 0.128173828125,
    'ãƒ¡ãƒªãƒƒãƒˆ': 0.12384033203125,
    'ã™ã™ã‚': 0.10467529296875,
    'æ¸›': 0.084228515625,
    'è»½æ¸›': 0.07159423828125,
    'æ‰‹æ®µ': 0.0250701904296875,
    'åŠ¹æžœ': 0.022216796875,
    'ã‚¬ã‚¹': 0.0019512176513671875},
]]}
```

### OpenSearch ã®è¨­å®š
OpenSearch ã® Neural sparse search ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãªãã€Sparse search ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®å‘¼ã³å‡ºã—ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¯ã€OpenSearch ãŒæŽ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¨é€£æºã—ã¦å®Ÿæ–½ã—ã¦ãã‚Œã‚‹ãŸã‚ã§ã™ã€‚ä»¥ä¸‹ã¯ãƒ¢ãƒ‡ãƒ«ã¨ OpenSearch å†…ã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å¯¾å¿œå›³ã§ã™ã€‚

![](https://storage.googleapis.com/zenn-user-upload/9cbbb853f418-20241227.png)

Sparse neural search ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½œæˆã—ã¦ã„ãã¾ã™ã€‚ä»¥é™ã®ä½œæ¥­ã‚‚ JupyterLab(ã‚‚ã—ãã¯ Jupyter Notebook) ä¸Šã§å®Ÿè¡Œã—ã¦ã„ãã¾ã™ã€‚

1. ã‚³ãƒã‚¯ã‚¿ãƒ¼(Connector) - æŽ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãªã©ã€ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™ãŸã‚ã«å¿…è¦ãªæƒ…å ±ã‚’æŒã¤ã‚‚ã®
2. ãƒ¢ãƒ‡ãƒ«(Model) - OpenSearch ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚«ã‚¿ãƒ­ã‚°ã«ç™»éŒ²ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã€‚ã‚³ãƒã‚¯ã‚¿ãƒ¼ã‹ã‚‰ãªã‚‹
3. å–ã‚Šè¾¼ã¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³(Ingest Pipeline) - ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿æ™‚ã«å¤–éƒ¨ã®æŽ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
4. æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³(Search Pipeline) - æ¤œç´¢æ™‚ã«å¤–éƒ¨ã®æŽ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—ã€ã‚¯ã‚¨ãƒªã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹(Index) - Sparse search ã«å¿…è¦ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’æ ¼ç´

#### äº‹å‰å‡¦ç†
ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆãªã©å®Ÿæ–½ã—ã¦ã„ãã¾ã™ã€‚

```
!pip install opensearch-py requests-aws4auth 'awswrangler[opensearch]' --quiet
```

```
import boto3
import json
import time
import awswrangler as wr
import pandas as pd
import numpy as np
from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth

def search_sagemaker_inference_endpoint(model_id, region):
    sagemaker_client = boto3.client("sagemaker", region_name=region)
    response = sagemaker_client.search(
        Resource="Endpoint",
        SearchExpression={
            "Filters": [
                {
                    "Name": "EndpointName",
                    "Operator": "Contains",
                    "Value": model_id
                },
            ],
        },
        SortBy="LastModifiedTime",
        SortOrder="Descending",
        MaxResults=1,
    )
    return response["Results"]

default_region = boto3.Session().region_name
```

ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ã‚»ãƒƒãƒˆã—ã¾ã™ã€‚

```
opensearch_cluster_endpoint = "search-sample-qrvg3nyg1vxb-ox9pmxoel6g5zjegp7cvdyfuca.us-east-1.es.amazonaws.com"
```

```
credentials = boto3.Session().get_credentials()
service_code = "es"
auth = AWSV4SignerAuth(credentials=credentials, region=default_region, service=service_code)
opensearch_client = OpenSearch(
    hosts=[{"host": opensearch_cluster_endpoint, "port": 443}],
    http_compress=True, 
    http_auth=auth,
    use_ssl=True,
    verify_certs=True,
    connection_class = RequestsHttpConnection
)

opensearch_client.info()
```

#### æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã¾ã™ã€‚

```
payload = {
  "mappings": {
    "properties": {
      "id": {"type": "keyword"},
      "question": {"type": "text", "analyzer": "custom_sudachi_analyzer"},
      "context":  {"type": "text", "analyzer": "custom_sudachi_analyzer"},
      "answers":  {"type": "text", "analyzer": "custom_sudachi_analyzer"},
      "question_sparse_embedding": {
        "type": "rank_features"
      },
    }
  },
  "settings": {
    "index.knn": True,
    "index.number_of_shards": 1,
    "index.number_of_replicas": 0,
    "analysis": {
      "analyzer": {
        "custom_sudachi_analyzer": {
          "char_filter": ["icu_normalizer"],
          "filter": [
              "sudachi_normalizedform",
              "custom_sudachi_part_of_speech"
          ],
          "tokenizer": "sudachi_tokenizer",
          "type": "custom"
        }
      },
      "filter": {
        "custom_sudachi_part_of_speech": {
          "type": "sudachi_part_of_speech",
          "stoptags": ["æ„Ÿå‹•è©ž,ãƒ•ã‚£ãƒ©ãƒ¼","æŽ¥é ­è¾ž","ä»£åè©ž","å‰¯è©ž","åŠ©è©ž","åŠ©å‹•è©ž","å‹•è©ž,ä¸€èˆ¬,*,*,*,çµ‚æ­¢å½¢-ä¸€èˆ¬","åè©ž,æ™®é€šåè©ž,å‰¯è©žå¯èƒ½"]
        }
      }
    }
  }
}
# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åã‚’æŒ‡å®š
index_name = "jsquad"

try:
    # æ—¢ã«åŒåã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ã„ã£ãŸã‚“å‰Šé™¤ã‚’è¡Œã†
    print("# delete index")
    response = opensearch_client.indices.delete(index=index_name)
    print(json.dumps(response, indent=2))
except Exception as e:
    print(e)

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
response = opensearch_client.indices.create(index_name, body=payload)
response
```

#### ã‚³ãƒã‚¯ã‚¿ã®ä½œæˆã¨ãƒ¢ãƒ‡ãƒ«ã®ç™»éŒ²

Amazon SageMaker æŽ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®æƒ…å ±ã‹ã‚‰ URL ã‚’ä½œæˆã—ã€ã‚³ãƒã‚¯ã‚¿ã®ç™»éŒ²ã¨ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```
hf_model_id_sparse_embedding = "hotchpotch/japanese-splade-v2"
model_id_sparse_embedding = hf_model_id_sparse_embedding.lower().replace("/", "-")

model_endpoint_name_sparse_embedding = search_sagemaker_inference_endpoint(model_id_sparse_embedding, default_region)[0]["Endpoint"]["EndpointName"]
model_endpoint_url_sparse_embedding = f"https://runtime.sagemaker.{default_region}.amazonaws.com/endpoints/{model_endpoint_name_sparse_embedding}/invocations"

print("embedding endpoint name: " + model_endpoint_name_sparse_embedding)
print("embedding model endpoint url: " + model_endpoint_url_sparse_embedding)

payload = {
  "name": model_id_sparse_embedding,
  "description": "Remote connector for "+ model_id_sparse_embedding,
  "version": 1,
  "protocol": "aws_sigv4",
  "credential": {
    "roleArn": opensearch_connector_role_arn
  },
  "parameters": {
    "region": default_region,
    "service_name": "sagemaker"
  },
  "actions": [
    {
      "action_type": "predict",
      "method": "POST",
      "headers": {
        "content-type": "application/json"
      },
      "url": model_endpoint_url_sparse_embedding,
      "pre_process_function": """
        def text_docs = params.text_docs;
        def textDocsBuilder = new StringBuilder('[');
        for (int i=0; i<text_docs.length; i++) {
          textDocsBuilder.append('\"');
          textDocsBuilder.append(text_docs[i]);
          textDocsBuilder.append('\"');
          if (i<text_docs.length - 1) {
            textDocsBuilder.append(',');
          }
        }
        textDocsBuilder.append(']');
        def parameters = '{ \"inputs\": ' + textDocsBuilder.toString() + ' }';
        return  '{\"parameters\": ' + parameters + '}';
      """,
      "request_body": "{ \"inputs\": ${parameters.inputs}}"
    }
  ]
}

response = opensearch_client.http.post("/_plugins/_ml/connectors/_create", body=payload)
sparse_embedding_connector_id = response['connector_id']
print("sparse embedding connector id: " + sparse_embedding_connector_id)

payload = {
    "name": model_id_sparse_embedding,
    "description": model_id_sparse_embedding,
    "function_name": "remote",
    "connector_id": sparse_embedding_connector_id
}
response = opensearch_client.http.post("/_plugins/_ml/models/_register?deploy=true", body=payload)

opensearch_model_id_sparse_embedding = response['model_id']

for i in range(300):
    ml_model_status = opensearch_client.http.get("/_plugins/_ml/models/"+ opensearch_model_id_sparse_embedding)
    model_state = ml_model_status.get("model_state")
    if model_state in ["DEPLOYED", "PARTIALLY_DEPLOYED"]:
        break
    time.sleep(1)

print(ml_model_status)

if model_state == "DEPLOYED":
    print("sparse embedding model " + opensearch_model_id_sparse_embedding + " is deployed successfully")
elif model_state == "PARTIALLY_DEPLOYED":
    print("sparse embedding model " + opensearch_model_id_sparse_embedding + " is deployed only partially")
else:
    raise Exception("sparse embedding model " + opensearch_model_id_sparse_embedding + " deployment failed")
```

ä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ« ID ã¯å¾Œç¶šã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆæ™‚ã«å¿…è¦ã«ãªã‚Šã¾ã™ã€‚

```
{'name': 'hotchpotch-japanese-splade-v2', 'model_group_id': 'Dy8s95MB2SlQmDGVnqrm', 'algorithm': 'REMOTE', 'model_version': '1', 'description': 'hotchpotch-japanese-splade-v2', 'model_state': 'DEPLOYED', 'created_time': 1735018716938, 'last_updated_time': 1735018716991, 'last_deployed_time': 1735018716991, 'auto_redeploy_retry_times': 0, 'planning_worker_node_count': 1, 'current_worker_node_count': 1, 'planning_worker_nodes': ['zZiZmVlXSwqGMPV4yCjBeg'], 'deploy_to_all_nodes': True, 'is_hidden': False, 'connector_id': 'Di8s95MB2SlQmDGVk6pE'}
sparse embedding model ES8s95MB2SlQmDGVn6oK is deployed successfully
```

ãƒ¢ãƒ‡ãƒ«ãŒç™»éŒ²å‡ºæ¥ãŸã‚‰ã€OpenSearch çµŒç”±ã§ãƒ¢ãƒ‡ãƒ«ã®å‘¼ã³å‡ºã—ãŒè¡Œãˆã‚‹ã‹ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚

```
path = "/_plugins/_ml/_predict/sparse_encoding/" + opensearch_model_id_sparse_embedding

payload = {
    "text_docs": ["æ—¥æœ¬ã§æ¢…é›¨ãŒãªã„ã®ã¯ã©ã“ï¼Ÿ"]
}

opensearch_client.http.post(path, body=payload)
```

ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãç™»éŒ²ã§ãã¦ã„ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªçµæžœãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

```
{'inference_results': [{'output': [{'name': 'response',
     'dataAsMap': {'response': [{'æ—¥æœ¬': 1.380859375,
        '##é›¨': 1.1826171875,
        'ãªã„': 1.0830078125,
        'å ´æ‰€': 1.0546875,
        'æ¢…': 1.0205078125,
        'ãªã—': 0.90380859375,
        'å›½': 0.7978515625,
        'é›¨': 0.6806640625,
        'æ°—å€™': 0.67138671875,
        'å°‘ãªã„': 0.3505859375,
        'ç”º': 0.331787109375,
        'åœ°åŸŸ': 0.242431640625,
        'å­£ç¯€': 0.20263671875,
        'éƒ½å¸‚': 0.14013671875,
        'ä¸': 0.042999267578125}]}}],
   'status_code': 200}]}
```

#### ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä½œæˆ
ã‚¤ãƒ³ãƒ‡ã‚¯ã‚·ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã—ã¦ã„ãã¾ã™

ã‚¤ãƒ³ãƒ‡ã‚¯ã‚·ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†…ã§ã¯ã€Sparse Encoding ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã®ç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚
```
# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã®ä½œæˆ
payload = {
  "processors": [
    {
      "sparse_encoding": {
        "model_id": question_sparse_embedding
        }
      }
    }
  ]
}
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ID ã®æŒ‡å®š
indexing_embedding_pipeline_id = "indexing_embedding_pipeline"
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ API ã®å‘¼ã³å‡ºã—
response = opensearch_client.http.put("/_ingest/pipeline/" + indexing_embedding_pipeline_id, body=payload)
response
```

æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†…ã§ã¯ã€neural_query_enricher ãŠã‚ˆã³ neural_sparse_two_phase_processor ã® 2 ã¤ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚¯ã‚¨ãƒªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã‚’è¡Œã„ã¾ã™ã€‚

```
payload={
  "request_processors": [
    {
      "neural_sparse_two_phase_processor": {
        "tag": "neural-sparse",
        "description": "Creates a two-phase processor for neural sparse search."
      }
    },
    {
      "neural_query_enricher" : {
        "default_model_id": opensearch_model_id_sparse_embedding
        }
      }
    }
  ]
}
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ID ã®æŒ‡å®š
search_embedding_pipeline_id = "search_embedding_pipeline"
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ API ã®å‘¼ã³å‡ºã—
response = opensearch_client.http.put("/_search/pipeline/" + search_embedding_pipeline_id, body=payload)
response
```

#### ãƒ‡ãƒ¼ã‚¿ç™»éŒ²

ä½œæˆã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æŒ‡å®šã—ã¦ OpenSearch ã«ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ã‚’è¡Œã„ã¾ã™ã€‚å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‹ã‚‰å„ç¨®ã‚³ãƒã‚¯ã‚¿ã‚’é€šã˜ã¦ ML ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‘¼ã³å‡ºã—ã€åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ãªãŒã‚‰ãƒ‡ãƒ¼ã‚¿æ ¼ç´ãŒè¡Œã‚ã‚Œã¾ã™ã€‚

ä»Šå›žã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ [JSQuAD](https://github.com/yahoojapan/JGLUE/tree/main/datasets/jsquad-v1.1) ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

```
%%time
dataset_dir = "./dataset/jsquad/"
%mkdir -p $dataset_dir
!curl -L -s -o $dataset_dir/valid.json https://github.com/yahoojapan/JGLUE/raw/main/datasets/jsquad-v1.1/valid-v1.1.json 
#!curl -L -s -o $dataset_dir/train.json https://github.com/yahoojapan/JGLUE/raw/main/datasets/jsquad-v1.1/train-v1.1.json
```

```
%%time
import pandas as pd
import json

def squad_json_to_dataframe(input_file_path, record_path=["data", "paragraphs", "qas", "answers"]):
    file = json.loads(open(input_file_path).read())
    m = pd.json_normalize(file, record_path[:-1])
    r = pd.json_normalize(file, record_path[:-2])

    idx = np.repeat(r["context"].values, r.qas.str.len())
    m["context"] = idx
    m["answers"] = m["answers"]
    m["answers"] = m["answers"].apply(lambda x: np.unique(pd.json_normalize(x)["text"].to_list()))
    return m[["id", "question", "context", "answers"]]

valid_filename = f"{dataset_dir}/valid.json"
valid_df = squad_json_to_dataframe(valid_filename)

#train_filename = f"{dataset_dir}/train.json"
#train_df = squad_json_to_dataframe(train_filename)
```

```
%%time
index_name = "jsquad"
response = wr.opensearch.index_df(
    client=opensearch_client,
    df=valid_df,
    #df=pd.concat([train_df, valid_df]),
    use_threads=True,
    id_keys=["id"],
    index=index_name,
    bulk_size=10, # 10 ä»¶ãšã¤æ›¸ãè¾¼ã¿
    refresh=False,
    pipeline=indexing_embedding_pipeline_id
)

index_name = "jsquad"
response = opensearch_client.indices.refresh(index=index_name)
response = opensearch_client.indices.forcemerge(index=index_name)
```

### ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã¨ Neural sparse search ã«ã‚ˆã‚‹ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã®æ¯”è¼ƒ
#### ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã®ãƒ’ãƒƒãƒˆçŽ‡ã¯æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«æ ¼ç´ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å†…å®¹ã€ãŠã‚ˆã³ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã«ã‚ˆã‚‹æ­£è¦åŒ–è¨­å®šã«ã‚ˆã‚Šå·¦å³ã•ã‚Œã¾ã™ã€‚
ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã§ã¯ã€ä¸è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯æ¥µåŠ›å«ã¾ã‚Œãªã„æ–¹ãŒã‚ˆã„çµæžœã‚’å¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªå˜èªžã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹æ¤œç´¢ã§ã¯ååˆ†æ€§èƒ½ã‚’ç™ºæ®ã§ãã‚‹ã¨ã„ãˆã¾ã™ã€‚

**ã‚¯ã‚¨ãƒª**
```
index_name = "jsquad"

payload = {
  "query": {
    "match": {
      "question": {
        "query": "æ—¥æœ¬ æ¢…é›¨ ãªã„ ã©ã“",
        "operator": "and"
      }
    }
  },
  "_source": False,
  "fields": ["question", "answers", "context"],
  "size": 10
}

response = opensearch_client.search(
    index=index_name,
    body=payload
)

pd.json_normalize(response["hits"]["hits"])
```

**å®Ÿè¡Œçµæžœ**

| index | id | score | fields.question | fields.answers | fields.context |
|-------|-----|--------|----------------|----------------|----------------|
| jsquad | a10336p0q0 | 14.434446 | æ—¥æœ¬ã§æ¢…é›¨ãŒãªã„ã®ã¯åŒ—æµ·é“ã¨ã©ã“ã‹ã€‚ | å°ç¬ åŽŸè«¸å³¶, å°ç¬ åŽŸè«¸å³¶ã‚’é™¤ãæ—¥æœ¬ | æ¢…é›¨ [SEP] æ¢…é›¨ï¼ˆã¤ã‚†ã€ã°ã„ã†ï¼‰ã¯ã€åŒ—æµ·é“ã¨å°ç¬ åŽŸè«¸å³¶ã‚’é™¤ãæ—¥æœ¬ã€æœé®®åŠå³¶å—éƒ¨ã€ä¸­å›½... |
| jsquad | a10336p24q1 | 14.434446 | æ¢…é›¨ãŒæ—¥æœ¬ã®ä¸­ã§ãªã„åœ°åŸŸã¯ã©ã“ã‹ã€‚ | åŒ—æµ·é“, æ±åŒ—åœ°æ–¹ | æ¢…é›¨ [SEP] å¹´ã«ã‚ˆã£ã¦ã¯æ¢…é›¨æ˜Žã‘ã®æ™‚æœŸãŒç‰¹å®šã§ããªã‹ã£ãŸã‚Šã€ã‚ã‚‹ã„ã¯ç™ºè¡¨ãŒã•ã‚Œãªã„ã“... |

ã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã§ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ã€‚

```
index_name = "jsquad"

payload = {
  "query": {
    "match": {
      "question": {
        "query": "æ—¥æœ¬ã§æ¢…é›¨ãŒãªã„å ´æ‰€ã¯ï¼Ÿ",
        "operator": "and"
      }
    }
  },
  "_source": False,
  "fields": ["question", "answers", "context"],
  "size": 10
}

response = opensearch_client.search(
    index=index_name,
    body=payload
)

response
```

```
{'took': 1,
 'timed_out': False,
 '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},
 'hits': {'total': {'value': 0, 'relation': 'eq'},
  'max_score': None,
  'hits': []}}
```

ãƒ’ãƒƒãƒˆã—ã¾ã›ã‚“ã§ã—ãŸã€‚"ã¨ã“ã‚" ã§ã¯ãªã "å ´æ‰€" ã¨ã—ãŸã“ã¨ã§ãƒ’ãƒƒãƒˆã—ãªããªã£ã¦ã—ã¾ã£ãŸã“ã¨ãŒä¸»ãªè¦å› ã§ã™ã€‚ã„ã‚ã‚†ã‚‹ "ã¦ã«ã‚’ã¯" ã®ã‚ˆã†ãªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¯ãƒˆãƒ¼ã‚¯ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚Šè½ã¨ã•ã‚Œã¦æ¤œç´¢ã®å¯¾è±¡å¤–ã¨ãªã£ã¦ã„ã‚‹ãŸã‚ã€å®Ÿã¯ä¸Šè¨˜ã®ã‚¯ã‚¨ãƒªã¯ "æ—¥æœ¬" "æ¢…é›¨" "ãªã„" "å ´æ‰€" ã§ã®æ¤œç´¢ã¨ã»ã¼åŒã˜å½¢ã§å‡¦ç†ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã® 4 ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ãƒžãƒƒãƒã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Œã°çµæžœã¨ã—ã¦è¿”å´ã•ã‚ŒãŸã¯ãšã§ã™ãŒã€åŽ³å¯†ã«ãƒžãƒƒãƒã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒãªã„ãŸã‚ã‚¼ãƒ­ä»¶ãƒ’ãƒƒãƒˆã¨ãªã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚

#### Neural sparse search
Neural sparse search ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ä¸Žãˆã‚‰ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’é€šã˜ã¦ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ã“ã¨ã§æ¤œç´¢ã‚’è¡Œã„ã¾ã™ã€‚

ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ãŒ match ãªã©ã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã®ã¨ã¯ç•°ãªã‚Šã€Neural sparse search ã§ã¯ "neural_sparse" ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢ã‚’è¡Œã„ã¾ã™ã€‚

```
%%time
# search
index_name = "jsquad"
query = "æ—¥æœ¬ã§æ¢…é›¨ãŒãªã„å ´æ‰€ã¯ï¼Ÿ"
payload = {
  "size": 10,
  "query": {
    "neural_sparse": {
      "question_sparse_embedding": {
        "query_text": query,
      }
    }
  },
  "_source" : False,
  "fields": ["question", "answers",  "context"]
}
# æ¤œç´¢ API ã‚’å®Ÿè¡Œ
response = opensearch_client.search(
    body = payload,
    index = index_name,
    filter_path = "hits.hits",
    search_pipeline = search_embedding_pipeline_id #ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›ã‚’è¡Œã†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æŒ‡å®š
)

# çµæžœã‚’è¡¨ç¤º
pd.json_normalize(response["hits"]["hits"])
```

çµæžœã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚ä¸Šä½ 5 ã¤ã¯è³ªå•ã®æ„å›³ã«æ²¿ã£ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãªã£ã¦ã„ã¾ã™ã€‚ä¸‹ä½ 5 ã¤ã¯è³ªå•ã®æ„å›³ã¨é›¢ã‚Œã¦ã„ã¾ã™ãŒã€ä¸Šä½ 5 ã¤ã¨ã‚¹ã‚³ã‚¢ã«å¤§ããªé–‹ããŒã‚ã‚‹ã“ã¨ã‚‚åˆ†ã‹ã‚Šã¾ã—ãŸã€‚

| index | id | score | fields.question | fields.answers | fields.context |
|-------|-----|--------|----------------|----------------|----------------|
| jsquad | a10336p24q1 | 23.883795 | æ¢…é›¨ãŒæ—¥æœ¬ã®ä¸­ã§ãªã„åœ°åŸŸã¯ã©ã“ã‹ã€‚ | åŒ—æµ·é“, æ±åŒ—åœ°æ–¹ | æ¢…é›¨ [SEP] å¹´ã«ã‚ˆã£ã¦ã¯æ¢…é›¨æ˜Žã‘ã®æ™‚æœŸãŒç‰¹å®šã§ããªã‹ã£ãŸã‚Šã€ã‚ã‚‹ã„ã¯ç™ºè¡¨ãŒã•ã‚Œãªã„ã“... |
| jsquad | a10336p0q0 | 21.338238 | æ—¥æœ¬ã§æ¢…é›¨ãŒãªã„ã®ã¯åŒ—æµ·é“ã¨ã©ã“ã‹ã€‚ | å°ç¬ åŽŸè«¸å³¶, å°ç¬ åŽŸè«¸å³¶ã‚’é™¤ãæ—¥æœ¬ | æ¢…é›¨ [SEP] æ¢…é›¨ï¼ˆã¤ã‚†ã€ã°ã„ã†ï¼‰ã¯ã€åŒ—æµ·é“ã¨å°ç¬ åŽŸè«¸å³¶ã‚’é™¤ãæ—¥æœ¬ã€æœé®®åŠå³¶å—éƒ¨ã€ä¸­å›½... |
| jsquad | a10336p32q3 | 19.378119 | æ¢…é›¨ãŒãªã„ã¨ã•ã‚Œã¦ã„ã‚‹éƒ½é“åºœçœŒã¯ã©ã“ï¼Ÿ | åŒ—æµ·é“ | æ¢…é›¨ [SEP] å®Ÿéš›ã®æ°—è±¡ã¨ã—ã¦ã¯åŒ—æµ·é“ã«ã‚‚é“å—ã‚’ä¸­å¿ƒã«æ¢…é›¨å‰ç·šãŒã‹ã‹ã‚‹ã“ã¨ã¯ã‚ã‚‹ãŒã€å¹³... |
| jsquad | a10336p32q2 | 17.695690 | æ°—å€™å­¦çš„ã«ã¯æ¢…é›¨ã¯ãªã„ã¨ã•ã‚Œã¦ã„ã‚‹å ´æ‰€ã¯ï¼Ÿ | åŒ—æµ·é“ | æ¢…é›¨ [SEP] å®Ÿéš›ã®æ°—è±¡ã¨ã—ã¦ã¯åŒ—æµ·é“ã«ã‚‚é“å—ã‚’ä¸­å¿ƒã«æ¢…é›¨å‰ç·šãŒã‹ã‹ã‚‹ã“ã¨ã¯ã‚ã‚‹ãŒã€å¹³... |
| jsquad | a10336p18q0 | 16.959581 | æ—¥æœ¬ã®åœ°åŸŸã§æœ¬æ ¼çš„ãªé•·é›¨ã«çªå…¥ã—ãªã„å ´æ‰€ã¯ã©ã“ã‹ã€‚ | åŒ—æµ·é“ | æ¢…é›¨ [SEP] æ¬¡ã«æ¢…é›¨å‰ç·šã¯ä¸­å›½ã®æ±Ÿæ·®ï¼ˆé•·æ±ŸæµåŸŸãƒ»æ·®æ²³æµåŸŸï¼‰ã«åŒ—ä¸Šã™ã‚‹ã€‚6æœˆä¸‹æ—¬ã«ã¯è¯... |
| jsquad | a10336p42q1 | 12.954581 | æ¢…é›¨ã®æœŸé–“ä¸­ã»ã¨ã‚“ã©é›¨ãŒé™ã‚‰ãªã„å ´åˆã‚’ä½•ã¨å‘¼ã¶ï¼Ÿ | ç©ºæ¢…é›¨, ç©ºæ¢…é›¨ï¼ˆã‹ã‚‰ã¤ã‚†ï¼‰ | æ¢…é›¨ [SEP] æ¢…é›¨ã®æœŸé–“ä¸­ã»ã¨ã‚“ã©é›¨ãŒé™ã‚‰ãªã„å ´åˆãŒã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ãªæ¢…é›¨ã®ã“ã¨ã‚’ç©ºæ¢…é›¨... |
| jsquad | a10336p42q2 | 12.952448 | æ¢…é›¨ã®æœŸé–“ä¸­ã»ã¨ã‚“ã©é›¨ãŒé™ã‚‰ãªã„å ´åˆã‚’ãªã‚“ã¨ã„ã†ï¼Ÿ | ç©ºæ¢…é›¨, ç©ºæ¢…é›¨ï¼ˆã‹ã‚‰ã¤ã‚†ï¼‰ | æ¢…é›¨ [SEP] æ¢…é›¨ã®æœŸé–“ä¸­ã»ã¨ã‚“ã©é›¨ãŒé™ã‚‰ãªã„å ´åˆãŒã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ãªæ¢…é›¨ã®ã“ã¨ã‚’ç©ºæ¢…é›¨... |
| jsquad | a10336p42q4 | 12.803707 | ã»ã¨ã‚“ã©é›¨ãŒé™ã‚‰ãªã„æ¢…é›¨ã‚’ä½•ã¨ã„ã†ï¼Ÿ | ç©ºæ¢…é›¨, ç©ºæ¢…é›¨ï¼ˆã‹ã‚‰ã¤ã‚†ï¼‰ | æ¢…é›¨ [SEP] æ¢…é›¨ã®æœŸé–“ä¸­ã»ã¨ã‚“ã©é›¨ãŒé™ã‚‰ãªã„å ´åˆãŒã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ãªæ¢…é›¨ã®ã“ã¨ã‚’ç©ºæ¢…é›¨... |
| jsquad | a10336p42q0 | 12.474874 | æ¢…é›¨ã®æœŸé–“ä¸­ã»ã¨ã‚“ã©é›¨ãŒé™ã‚‰ãªã„å ´åˆãŒã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ãªæ¢…é›¨ã®ã“ã¨ã‚’ãªã‚“ã¨ã„ã†ã‹ï¼Ÿ | ç©ºæ¢…é›¨, ç©ºæ¢…é›¨ï¼ˆã‹ã‚‰ã¤ã‚†ï¼‰ | æ¢…é›¨ [SEP] æ¢…é›¨ã®æœŸé–“ä¸­ã»ã¨ã‚“ã©é›¨ãŒé™ã‚‰ãªã„å ´åˆãŒã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ãªæ¢…é›¨ã®ã“ã¨ã‚’ç©ºæ¢…é›¨... |
| jsquad | a10336p7q4 | 11.680378 | æ¢…é›¨ã®äº‹ã‚’ä¸­å›½ã§ã¯ã€ä½•ã¨ã„ã†ã‹ã€‚ | ã€Œï¼ˆãƒ¡ã‚¤ãƒ¦ãƒ¼ï¼‰ã€, ãƒ¡ã‚¤ãƒ¦ãƒ¼ | æ¢…é›¨ [SEP] ä¸­å›½ã§ã¯ã€Œï¼ˆãƒ¡ã‚¤ãƒ¦ãƒ¼ï¼‰ã€ã€å°æ¹¾ã§ã¯ã€Œï¼ˆãƒ¡ã‚¤ãƒ¦ãƒ¼ï¼‰ã€ã‚„ã€ŒèŠ’ç¨®é›¨ã€ã€éŸ“å›½ã§ã¯... |


## ã¾ã¨ã‚
Sparse encoding ã«ã‚ˆã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹ã®ã‚ã„ã¾ã„æ¤œç´¢ã«ã‚ˆã‚Šã€å¾“æ¥åž‹ã®å…¨æ–‡æ¤œç´¢ã§ã¯ã‚«ãƒãƒ¼ã§ããªã„ã‚¯ã‚¨ãƒªã«ã‚ˆã‚‹ã‚¼ãƒ­ä»¶ãƒ’ãƒƒãƒˆã®èª²é¡Œã‚’è§£æ¶ˆã§ãã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸã€‚

Neural sparse search ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã§ç”Ÿæˆã™ã‚‹ã“ã¨ãªãã€é€šå¸¸ã®å…¨æ–‡æ¤œç´¢ã¨åŒã˜ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ•ã’ã‚‹ã ã‘ã§ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ãŒå®Ÿè¡Œã§ãã‚‹ã“ã¨ã‚‚ç¢ºèªã§ãã¾ã—ãŸã€‚OpenSearch ã®ã‚³ãƒã‚¯ã‚¿ã‚„ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ©Ÿèƒ½ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€OpenSearch å†…ã§åŸ‹ã‚è¾¼ã¿ã®å‡¦ç†ã‚’å®Œçµã§ãã‚‹ãŸã‚ã€å®Ÿè£…ã®é¸æŠžè‚¢ãŒåºƒãŒã‚Šã¾ã™ã€‚ã‚‚ã¡ã‚ã‚“ã€[ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´ã§ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã—ãŸã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢](https://opensearch.org/docs/latest/search-plugins/neural-sparse-with-raw-vectors/)ã‚’è¡Œã†ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

OpenSearch ã§ã¯ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã¨ã‚¹ãƒ‘ãƒ¼ã‚¹æ¤œç´¢ã®[ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢](https://opensearch.org/docs/latest/search-plugins/hybrid-search/)ã‚‚å¯èƒ½ã§ã‚ã‚‹ãŸã‚ã€ã“ã‚Œã‚‰ã®æ¤œç´¢ã‚’ä½µç”¨ã™ã‚‹ã“ã¨ã§ã‚ˆã‚Šå¹…åºƒã„ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«å¯¾å¿œã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

